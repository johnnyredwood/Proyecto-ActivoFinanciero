```Universidad San Francisco de Quito
Data Mining
Proyecto Activo Financiero
John Ochoa (00345743)

#Github Link:
https://github.com/johnnyredwood/Proyecto-ActivoFinanciero

#Descripci√≥n del proyecto:
Infraestructura anal√≠tica y de machine learning para un modelo de clasificaci√≥n orientado a decisiones de trading intrad√≠a/simple sobre activos financieros. El proyecto integra:
- Jupyter para consumo de datos raw
- Script para construcci√≥n automatizada de tabla de datos en esquema analytics
- Notebook con EDA, preparaci√≥n y entrenamiento del modelo.
- Simulaci√≥n de inversi√≥n (backtest) 2025 con reglas expl√≠citas y m√©tricas de desempe√±o.
- FastAPI para exponer el modelo v√≠a HTTP (`/health`, `/predict`) con Docker/Compose, montando el artefacto entrenado desde `libros/modelos`.

Los datos de activos financieros ser√°n directamente extra√≠dos de la API de Yahoo Finance para un rango de fechas especificado en las variables de entorno y a su vez identificado por los denominadores tickers de igual forma previamente indicados en el archivo .env (ver .env.example o descripci√≥n de variables de entorno del presente readme)

El modelo predice la direcci√≥n pr√≥xima del retorno (arriba/abajo) que es una variable target de tipo binario denominada target_up usando features derivadas (d√≠a de la semana, volatilidad reciente, retornos previos, volumen, etc.). La API devuelve √∫nicamente la etiqueta `pred_label` para facilitar su consumo y de igual forma simplificar el uso del modelo.

#Checklist de aceptaci√≥n
[x] Docker Compose levanta la API del modelo (FastAPI) correctamente.
[x] Variables sensibles y puertos gestionados v√≠a `.env` y `docker-compose.yaml`.
[x] Artefacto del modelo guardado en `libros/modelos` y montado en el contenedor.
[x] Notebook con EDA breve (balance de clases, distribuci√≥n de retornos).
[x] Balanceo de clases en Train (undersampling/oversampling configurable).
[x] Obtenci√≥n de mejor modelo a trav√©s de validaci√≥n y testing en 7 modelos de Machine Learning
[x] Simulaci√≥n 2025 con curva de equity, drawdown y comparaci√≥n con m√©tricas ML.
[x] README claro: pasos, variables, arquitectura, decisiones y troubleshooting.

#Variables de ambiente: listado y prop√≥sito; gu√≠a para .env

Variables principales del proyecto (ver `.env.example`):

JUPYTER_TOKEN=token1234
PYSPARK_PYTHON=python3
SPARK_LOCAL_IP=0.0.0.0
PORT_JUPYTER=8888
PORT_SPARK=4040
PORT_POSTGRES=5432
PORT_WAREHOUSEUI=8080
POSTGRES_HOST=postgres_host
POSTGRES_DB=activo_financiero
POSTGRES_USER=usuario_spark
POSTGRES_PASSWORD=usuario_password
PGADMIN_DEFAULT_EMAIL=admin@admin.com
PGADMIN_DEFAULT_PASSWORD=adminPassword123
TICKERS= AAPL,MSFT,SPY,NVDA,INTC,AMD,GOOGL,AMZN,TSLA,JPM
START_DATE=01-01-2020
END_DATE=30-11-2025
RAW_TABLE=prices_daily
ANALYTICS_TABLE=daily_features
API_PORT=7000

Notas:
- La API usa por defecto el camino del contenedor `/models/best_pipeline.joblib`. Compose monta `MODEL_DIR` del host en `/models` para que el archivo est√© disponible.
- Se puede ajustar `API_PORT` si se desea exponer otro puerto local.

#Gu√≠a de variables de entorno (.env)

- `JUPYTER_TOKEN`: token para acceso a Jupyter dentro del contenedor.
- `PYSPARK_PYTHON`: int√©rprete Python que usar√° PySpark (`python3`).
- `SPARK_LOCAL_IP`: IP local de Spark para UI y jobs (`0.0.0.0`).
- `PORT_JUPYTER`: puerto de Jupyter Notebook (por ejemplo `8888`).
- `PORT_SPARK`: puerto de Spark UI (por ejemplo `4040`).
- `PORT_POSTGRES`: puerto del servicio Postgres (por ejemplo `5432`).
- `PORT_WAREHOUSEUI`: puerto de UI/pgAdmin (por ejemplo `8080`).
- `POSTGRES_HOST`: hostname del contenedor/database para conexi√≥n (`postgres_host`).
- `POSTGRES_DB`: nombre de la base de datos (`activo_financiero`).
- `POSTGRES_USER`: usuario de conexi√≥n (ejemplo `usuario_spark`).
- `POSTGRES_PASSWORD`: contrase√±a del usuario de conexi√≥n.
- `PGADMIN_DEFAULT_EMAIL`: email por ejemplo de pgAdmin.
- `PGADMIN_DEFAULT_PASSWORD`: contrase√±a por ejemplo de pgAdmin.
- `TICKERS`: lista separada por comas de s√≠mbolos a descargar (ej. `AAPL,MSFT,SPY,...`).
- `START_DATE`: fecha inicial (formato `DD-MM-YYYY`, ej. `01-01-2020`).
- `END_DATE`: fecha final (formato `DD-MM-YYYY`, ej. `30-11-2025`).
- `RAW_TABLE`: nombre de tabla RAW para precios diarios (`prices_daily`).
- `ANALYTICS_TABLE`: nombre de tabla de features derivadas (`daily_features`).
- `API_PORT`: puerto local para la API de FastAPI (por ejemplo `7000`).

#Arquitectura

 Inicializaci√≥n de esquemas raw y analytics
        ‚îÇ
        ‚ñº
 Notebook yf_ingesta.ipynb para ingesta de datos hacia RAW
        ‚îÇ
        ‚ñº
 Script build_features.py para construcci√≥n de tabla hacia ANALYTICS
        ‚îÇ
        ‚ñº
 Preparaci√≥n y Entrenamiento (EDA + balanceo + pipeline)
        ‚îÇ
        ‚ñº
 Obtenci√≥n del mejor modelo (`joblib`) y simulaci√≥n de trading
        ‚îÇ
        ‚ñº
 Serving con FastAPI del mejor modelo bajo endpoint predict

#Estructura de directorios

üìÅ init-scripts             -> SQL inicial y esquemas
‚îÇ   ‚îî‚îÄ‚îÄ 01-init-schemas.sql
üìÅ libros                   -> Notebooks y artefactos del modelo de ingesta + ML
‚îÇ   ‚îú‚îÄ‚îÄ yf_ingesta.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ ml_trading_classifier.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ modelos/            -> Artefacto del modelo (`best_pipeline.joblib`)
üìÅ model-api                -> C√≥digo de la API FastAPI (serving)
üìÅ models                   -> Carpeta auxiliar de modelos (si aplica)
üìÅ scripts                  -> Scripts utilitarios (ETL/Features/OBT si aplica)
‚îÇ   ‚îî‚îÄ‚îÄ build_features.py
üìÅ warehouse_data           -> Datos persistidos (volumenes/BD)
üìÅ warehouse_ui_data        -> Datos de UI (pgAdmin/sesiones)
docker-compose.yaml         -> Orquestaci√≥n de servicios (API, montajes)
Dockerfile.feature-builder  -> Dockerfile para construir/servir la API
README.md                   -> Documentaci√≥n del proyecto
requirements.txt            -> Dependencias Python

#Pasos de ejecuci√≥n (end-to-end)

Prerrequisitos
* Docker y Docker Compose instalados
* Archivo `.env` configurado con variables de datos y puertos

1) Clonar y configurar

- Clonar el repositorio:
git clone https://github.com/johnnyredwood/Proyecto-ActivoFinanciero


- Crear `.env` desde plantilla y ajustar valores:
cp .env.example .env


2) Levantar infraestructura base

- En la ra√≠z del proyecto, levantar contenedores base:
docker-compose --env-file .env up -d
docker-compose ps


3) Ingesta de datos RAW (Yahoo Finance)

- Ejecutar el notebook `libros/yf_ingesta.ipynb` dentro de Jupyter para poblar la tabla RAW (`RAW_TABLE`, por defecto `prices_daily`).
- Acceso a Jupyter (si est√° habilitado): `http://localhost:${PORT_JUPYTER}` con token `${JUPYTER_TOKEN}`.

4) Construcci√≥n de tabla ANALYTICS (features)

- Ejecutar el constructor de features con par√°metros del `.env` se puede utilizar el modo full o modo 

docker compose run --rm 
       -e RAW_TABLE=prices_daily 
       -e ANALYTICS_TABLE=daily_features 
       feature-builder 
       /app/scripts/build_features.py --mode full 
       --ticker AAPL,MSFT,SPY,NVDA,INTC,AMD,GOOGL,AMZN,TSLA,JPM 
       --start-date 2020-01-01 
       --end-date 2025-11-30 
       --run-id full_load 
       --overwrite true 
       --vol-window 20

Par√°metros del constructor de features (`build_features.py`):
- `--mode` (obligatorio):
       - `full`: procesa el rango completo definido por `--start-date` y `--end-date` para todos los tickers provistos.
       - `by-date-range`: procesa exactamente el rango indicado (√∫til para cargas parciales o incrementales).
- `--ticker` (obligatorio): lista separada por comas de s√≠mbolos. Ej: `AAPL,MSFT,SPY`.
- `--start-date` (obligatorio): fecha inicial en formato `YYYY-MM-DD`. Ej: `2020-01-01`.
- `--end-date` (obligatorio): fecha final en formato `YYYY-MM-DD`. Ej: `2025-11-30`.
- `--run-id` (obligatorio): identificador de ejecuci√≥n para trazabilidad (ej. `full_load`, `daily_job_2025_11_30`).
- `--overwrite` (opcional, `true|false`, por defecto `false`):
       - `true`: reescribe particiones/registros existentes del rango/tickers indicados.
       - `false`: preserva registros existentes y solo inserta nuevos.
- `--vol-window` (opcional, entero, por defecto `20`): ventana en d√≠as para c√°lculo de volatilidad y estad√≠sticas m√≥viles.

Variables de ambiente utilizadas por el servicio:
- `RAW_TABLE`: nombre de la tabla RAW desde la que se leen precios (ej. `prices_daily`).
- `ANALYTICS_TABLE`: nombre de la tabla destino de features derivadas (ej. `daily_features`).

5) Entrenamiento y selecci√≥n de modelo

- Ejecutar el notebook `libros/ml_trading_classifier.ipynb` usando como fuente la tabla del esquema ANALYTICS.
- Se Exportar√° el mejor pipeline a `libros/modelos/best_pipeline.joblib`.

6) Construir y levantar la API del modelo

- Construir la imagen del servicio `model-api` y levantarlo:

docker compose build model-api
docker compose up -d model-api
docker compose ps

7) Probar la API

- Endpoint Health con ejemplo de llamada:

Invoke-RestMethod -Uri http://localhost:{API_PORT}/health

-UI para la API: abrir `http://localhost:{API_PORT}/docs` en el navegador.

- Endpoint Predict con ejemplo de llamada usando mis features seleccionadas:
Invoke-RestMethod -Method Post -Uri http://localhost:{API_PORT}/predict -ContentType 'application/json' -Body '{
       "year": 2025,
       "month": 6,
       "day_of_week": 3,
       "open": 150.23,
       "volume": 1234567,
       "return_prev_close": 0.0045,
       "volatility_n_days": 0.012,
       "is_monday": false,
       "is_friday": true
}'

Respuesta esperada:
{"pred_label": 1}

EDA y Modelado (resumen):

- EDA breve: distribuci√≥n de retornos (histogramas), balance de clases global y por ticker, rango temporal y shape del dataset.
- Balanceo de clases en Train: opci√≥n de undersampling de la mayor√≠a u oversampling de la minor√≠a; se utiliza `balanced_train_df` para entrenar.
- Pipeline: preprocesamiento de num√©ricas y categ√≥ricas, modelo de clasificaci√≥n (e.g., XGBoost/LightGBM/RandomForest), selecci√≥n por m√©tricas en validaci√≥n.
- Exportaci√≥n: `joblib.dump(pipeline, 'libros/modelos/best_pipeline.joblib')`.
- Features utilizados para evitar leakage: "year", "month", "day_of_week","open", "return_prev_close", "volatility_n_days", "volume","is_monday", "is_friday"
- Modelos utilizados para buscar mejor predictor (clasificaci√≥n binaria): Regresi√≥n Log√≠stica, Decision Tree, Random Forest, Gradient Boosting, AdaBoosting, XGBoosting, Light Gradient Boosting con eso se pudo decidir el mejor modelo de entre esos que para el caso de mis datos fue XGBoosting
- En el presente caso de mi proyecto se utiliza para el entrenamiento datos del a√±o anterior a 2023, para validaci√≥n datos del 2024 y para testing datos del 2025

#Simulaci√≥n 2025 (backtest)

Regla simple:
- Si `pred_label == 1`, comprar al open y cerrar al close del d√≠a; si `0`, estar en efectivo.

Outputs:
- Curva de equity del activo y portafolio (si varios tickers).
- Drawdown m√°ximo, retorno total y anualizado, n√∫mero de trades.
- Comparaci√≥n con m√©tricas ML (accuracy, precision/recall si aplica).

#Troubleshooting

- La API muestra `prob_up`: reconstruir la imagen y reiniciar Compose; hacer hard-refresh de `/docs` (Ctrl+F5). La respuesta final solo incluye `pred_label`.
- Error cargando modelo: confirmar que `libros/modelos` est√° montado en `/models` y que el archivo existe como `best_pipeline.joblib`.
- Puerto ocupado: cambiar `API_PORT` en `.env` y actualizar el mapeo en `docker-compose.yaml`.

#Conclusiones generales:

- Enfoque: predecimos si el d√≠a cerrar√° arriba del precio de apertura (`target_up`). Usamos solo datos disponibles antes de abrir el mercado para evitar errores por usar informaci√≥n futura.
- EDA: los datos son estables en el tiempo y hay d√≠as al alza y a la baja en proporciones razonables. Las estad√≠sticas muestran datos con se√±ales muy fluctuantes lo cual es l√≥gico en el sentido de que en el mercado hay continuos movimientos de alzas y bajas
- Features: usamos retornos previos, volatilidad calculada con ventanas, volumen y datos del calendario (d√≠a de la semana, mes). Todo se calcula con informaci√≥n que se puede tener disponible al inicio del d√≠a esto para evitar leakage.
- Splits: entrenamos con 2020‚Äì2023, validamos con 2024 y probamos con 2025 para medir qu√© tanto generaliza el modelo.
- Modelos: probamos varios y XGBoost fue el mejor, con buenas m√©tricas (F1 y ROC-AUC) y comportamiento m√°s estable.
- 2025: el modelo mantiene buen rendimiento fuera de muestra y sirve para una estrategia sencilla basada en la se√±al al inicio del d√≠a.
- Valor: el pipeline es reproducible y √∫til para decisiones t√°cticas de corto plazo; no reemplaza an√°lisis profundo del mercado ni expertise de traders.
- Futuro: mejorar el tuning, a√±adir features de microestructura, sentiment analysis, probar la idea en m√°s activos y con datos de m√°s a√±os.