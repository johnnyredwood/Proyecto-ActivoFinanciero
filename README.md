```Universidad San Francisco de Quito
Data Mining
Proyecto 05
John Ochoa Abad 345743

#Github Link:
https://github.com/johnnyredwood/PSET5-ENSEMBLE-REGRESSION

#DescripciÃ³n del proyecto:
Infraestructura analÃ­tica completa con Docker Compose integrando Jupyter+Spark y Postgres para procesar el dataset NYC TLC Trips 2015â€“2025. Se ingesta cobertura Yellow y Green en esquema RAW y se construye una tabla analÃ­tica unificada (One Big Table) en el esquema `analytics` mediante un Ãºnico comando (`build_obt.py`).

Sobre la OBT se realiza muestreo controlado y preparaciÃ³n de features (scaling numÃ©ricas + one-hot categÃ³ricas) para entrenar y comparar modelos de regresiÃ³n enfocÃ¡ndose en predecir `total_amount` al pickup. En esta versiÃ³n (PSET5) se evoluciona desde modelos lineales regularizados hacia un set de modelos ensemble (Voting, Bagging, Pasting, Gradient Boosting y LightGBM) mÃ¡s un baseline lineal, seleccionando el mejor por RMSE temporal en validaciÃ³n y auditando desempeÃ±o en test.

#Checklist de aceptaciÃ³n
[x] Docker Compose levanta Spark y Jupyter Notebook.
[x] Variables sensibles gestionadas vÃ­a archivo .env.
[x] Cobertura 2015â€“2025 (Yellow/Green) cargada en RAW con monitoreo por lote.
[x] Tabla `analytics.obt_trips` construida con columnas base, derivadas y metadatos.
[x] Muestreo controlado y particionado temporal (Train â‰¤2022 / Val 2023 / Test 2024).
[x] Modelos Ensemble (Voting, Bagging, Pasting, Gradient Boosting, LightGBM) comparados con baseline.
[x] SelecciÃ³n por menor RMSE en validaciÃ³n manteniendo MAE y RÂ² estables.
[x] README claro: pasos, variables, arquitectura, decisiones y troubleshooting.

#Variables de ambiente: listado y propÃ³sito; guÃ­a para .env.

Para la ejecuciÃ³n de los notebooks se definieron las siguientes variables de ambiente de Snowflake:

SOURCE_PATH=https://d37ci6vzurychx.cloudfront.net
YEARS=2020,2021,2022,2023,2024,2025
MONTHS=1,2,3,4,5,6,7,8,9,10,11,12
SERVICES=yellow,green
JUPYTER_TOKEN=token123
PYSPARK_PYTHON=python3
SPARK_LOCAL_IP=0.0.0.0
PORT_JUPYTER=8888
PORT_SPARK=4040
PORT_POSTGRES=5432
PORT_WAREHOUSEUI=8080
POSTGRES_HOST=postgres
POSTGRES_DB=database123
POSTGRES_USER=usuario123
POSTGRES_PASSWORD=clave123
PGADMIN_DEFAULT_EMAIL=admin@admin.com
PGADMIN_DEFAULT_PASSWORD=adminPassword123
PG_USER=usuario_pg123
PG_DB=database123

Con estas variables siguiendo el ejemplo del .env.example incluido en el proyecto se puede reproducir el mismo con credenciales propias
de esta manera se gestiona correctamente los datos sensibles.

#Arquitectura (flujo resumido)

 Ingesta Parquet (Yellow/Green & Zones)
        â”‚
        â–¼
 Esquema RAW (tablas particionadas + metadatos)
        â”‚
        â–¼
 ConstrucciÃ³n OBT (`analytics.obt_trips`)
        â”‚
        â–¼
 Muestreo & PreparaciÃ³n (limpieza, encoding, scaling)
        â”‚
        â–¼
 Entrenamiento Modelos (Baseline + Ensembles)
        â”‚
        â–¼
 SelecciÃ³n & EvaluaciÃ³n (ValidaciÃ³n / Test)


#Pasos para Docker Compose y ejecuciÃ³n de notebooks (incluido comando para construir OBT).

Prerrequisitos
*Docker instalado
*Docker Compose instalado
*Archivo .env configurado con las credenciales de Snowflake

1. Descargar de repositorio y ConfiguraciÃ³n del Ambiente

- Descargar el repositorio a su entorno local con
git clone https://github.com/johnnyredwood/PSET5-ENSEMBLE-REGRESSION/

Crear archivo de variables de ambiente:

- Copiar el template y configurar con valores reales
cp .env.example .env

- Editar el archivo .env con tus credenciales
nano .env

2. Verificar estructura de directorios (vista clave):

ðŸ“ drivers                  -> Dependencias externas (drivers JDBC, etc.)
ðŸ“ Evidencias               -> Capturas / artefactos de validaciÃ³n y resultados
ðŸ“ init-scripts             -> SQL inicial (esquemas, permisos) para Postgres
â”‚   â””â”€â”€ 01-init-schemas.sql
ðŸ“ libros                   -> Notebooks de ingesta y modelado
â”‚   ðŸ“ .ipynb_checkpoints    -> Estados intermedios automÃ¡ticos
â”‚   â”œâ”€â”€ 01_ingesta_parquet_raw.ipynb  -> Ingesta masiva RAW
â”‚   â”œâ”€â”€ pset5_ensemble_regression.ipynb -> Entrenamiento y comparaciÃ³n ensembles
â”‚   â”œâ”€â”€ checkpointTaxisGreen.json      -> Progreso ingesta Green
â”‚   â”œâ”€â”€ checkpointTaxisYellow.json     -> Progreso ingesta Yellow
â”‚   â””â”€â”€ postgresql-42.2.5.jar          -> Driver JDBC Postgres
ðŸ“ logs                     -> Logs operativos / seguimiento procesos
ðŸ“ scripts                  -> Scripts utilitarios (ETL / construcciÃ³n OBT)
â”‚   â””â”€â”€ build_obt.py        -> ConstrucciÃ³n tabla OBT parametrizada
ðŸ“ warehouse_data           -> Data directory Postgres (persistencia fÃ­sica)
ðŸ“ warehouse_ui_data        -> Data de la UI (pgAdmin / sesiones)
â”‚   â”œâ”€â”€ azurecredentialcache
â”‚   â”œâ”€â”€ sessions
â”‚   â”œâ”€â”€ storage
â”‚   â””â”€â”€ pgadmin4.db
.env                        -> Variables de entorno locales (no versionar sensibles)
.env.example                -> Plantilla de referencia para reproducir entorno
.gitignore                  -> Exclusiones de control de versiÃ³n
docker-compose.yaml         -> OrquestaciÃ³n de servicios (Spark, Jupyter, Postgres, pgAdmin)
Dockerfile.obt-builder      -> Imagen especializada para construcciÃ³n OBT
README.md                   -> DocumentaciÃ³n del proyecto
requirements.txt            -> Dependencias Python base

3. InicializaciÃ³n de la Infraestructura
Levantar los servicios con Docker Compose:

- Ejecutar en el directorio del proyecto el siguiente comando para levantar el contenedor con variables de entorno de .env
docker-compose --env-file .env up -d

- Verificar que el contenedor estÃ© corriendo
docker-compose ps

4. Acceder a Jupyter Notebook:
Acceder al Jupyter Notebook del contenedor con el puerto y token indicados en su .env

URL: http://localhost:puerto
Token: [valor de JUPYTER_TOKEN en .env]

5. EjecuciÃ³n Secuencial de Notebooks
Orden de ejecuciÃ³n obligatorio:

Notebook 01 - 01_ingesta_parquet_raw
ParÃ¡metros esperados:
- AÃ±os: 2015-2025 (configurado en .env)
- Meses: 1-12 (configurado en .env)  
- Servicios: yellow, green (configurado en .env)
Genera:
-Tabla RAW de datos de taxi por servicio en Snowflake

ConstrucciÃ³n de tabla OBT

Teniendo los contenedores corriendo en Docker desde consola ejecutar el siguiente comando:

docker compose run obt-builder python /app/scripts/build_obt.py --year-start yearInicio --year-end yearFin --services serviciosSeparadosPorComa --run-id identificadorRun --months mesesSeparadosPorEspacio

De donde:
yearInicio es el aÃ±o en formato entero desde el cual se quieren empezar a procesar los datos (verificar disponibilidad de datos de dicho aÃ±o en esquema Raw)
yearFin es el aÃ±o en formato entero hasta el cual se quieren procesar los datos (verificar disponibilidad de datos de dicho aÃ±o en esquema Raw)
serviciosSeparadosPorComa son los servicios de los taxis en este caso aplica yellow,green
identificadorRun es un identificador que se poblara en la tabla obt se puede ingresar cualquiera que desee el usuario
mesesSeparadosPorEspacio son los meses de cada aÃ±o que se deseen procesar en formato entero separados por espacios

Ejemplos de dicho comando para ejecutar son:

docker compose run obt-builder python /app/scripts/build_obt.py --year-start 2020 --year-end 2020 --services yellow,green --run-id full_load --months 3 4 5 6 7 8 9 10 11 12

docker compose run obt-builder python /app/scripts/build_obt.py --year-start 2022 --year-end 2022 --services yellow,green --run-id full_load

#DiseÃ±o de raw y OBT (columnas, derivadas, metadatos, supuestos).

*Esquema RAW
El esquema raw funciona como capa de aterrizaje donde se preservan los datos en su 
formato original con metadatos de ingesta. Se implementaron tablas particionadas por servicio 
y perÃ­odo para optimizar el manejo de los volÃºmenes de datos.

Estructura de tablas RAW:

NY_TAXI_RAW_YELLOW - Viajes de taxi amarillo RAW
NY_TAXI_RAW_GREEN - Viajes de taxi verde RAW
NY_TAXI_RAW_TAXI_ZONES - Zonas de Taxis de New York RAW

Columnas base preservadas del origen:

Datos temporales: pickup/dropoff datetime
Ubicaciones: PULocationID, DOLocationID
MÃ©tricas de viaje: trip_distance, passenger_count
Tarifas: fare_amount, tip_amount, tolls_amount, total_amount
Identificadores: VendorID, RatecodeID, payment_type

Metadatos de ingesta agregados:

run_id - Identificador Ãºnico de la ejecuciÃ³n
source_year / source_month - PerÃ­odo de origen
ingested_at_utc - Timestamp de ingesta
service_type - Tipo de servicio (yellow/green)

*Esquema ANALYTICS - OBT
La OBT consolida todos los datos de viajes de taxis de New York en una tabla que junta toda 
la informaciÃ³n necesaria validada y depurada a manera de ejecutar consultas de negocio
sobre la misma

Columnas de la OBT:

Temporales:
pickup_datetime, dropoff_datetime - Timestamps originales
pickup_date, pickup_hour - Componentes temporales
dropoff_date, dropoff_hour - Componentes temporales
trip_duration_min - DuraciÃ³n calculada en minutos

Ubicaciones:
pu_location_id, do_location_id - IDs originales
pu_zone, pu_borough - Nombres desnormalizados
do_zone, do_borough - Nombres desnormalizados

Servicio y CÃ³digos:
vendor_id, vendor_name - Desnormalizado
rate_code_id, rate_code_desc - Desnormalizado
payment_type, payment_type_desc - Desnormalizado

MÃ©tricas y Tarifas:
passenger_count, trip_distance
fare_amount, extra, mta_tax, tip_amount
tolls_amount, improvement_surcharge
congestion_surcharge, airport_fee, total_amount

Metadatos:
run_id - Trazabilidad de la ejecuciÃ³n
ingested_at_utc - Fecha de procesamiento
source_service - Servicio de origen
source_year, source_month - PerÃ­odo origen

Supuestos de DiseÃ±o
Clave Natural: Se define basada en pickup_datetime, PULocationID, DOLocationID y VendorID para garantizar identificaciÃ³n Ãºnica de viajes en merges

Estrategia de Idempotencia: ImplementaciÃ³n de UPSERT basado en clave natural, permitiendo reingesta sin duplicados.

Manejo de Datos: Se han filtrado nulos en campos obligatorios y se ha definido validaciones lÃ³gicas para datos nÃºmericos de forma que los mismos
cumplan con rangos lÃ³gicos

#Calidad/auditorÃ­a: quÃ© se valida y dÃ³nde se ve.

*ValidaciÃ³n de Conectividad con Snowflake desde Spark:
Inicio sesiÃ³n de Spark y posteriormente genero una conexiÃ³n con Snowflake con mis credenciales y ejecuto una query simple de SELECT current_version()
esto lo valido en todos los notebooks antes de proceder con el consumo, procesamiento y/o lectura de datos

*ValidaciÃ³n de Ingesta de datos:
En todos los notebooks he implementado logs en forma de prints y manejo de excepciones para ir monitoreando el proceso de consumo de todos los datos.
A su vez una vez los mismos se iban consumiendo ingresaba en Snowflake a verificar que las tablas aumenten en cantidad de filas y monitoreaba los datos
recien ingresados con queries simples desde Snowflake

*ValidaciÃ³n del contenedor de docker:
Al tener spark-notebook: Jupyter+Spark desde un contenedor de docker verificaba que el mismo estuviera funcionando correctamente con el comando docker ps,
con el Docker Desktop verificando que el contenedor este arriba e ingresando a localhost con el puerto definido y verificando que pudiera ingresar
sin problema a Jupyter

*Comentarios respecto a modelos ML:
\n+Enfoque actualizado (PSET5 - Modelos Ensemble)\n+\n+Se migrÃ³ del enfoque de modelos lineales regularizados (SGD, Ridge, Lasso, ElasticNet desde cero y sklearn) hacia un conjunto de modelos ensemble y comparativos para mejorar capacidad predictiva sobre `total_amount` y robustez temporal.\n+\n+Modelos incluidos:\n+* Baseline: RegresiÃ³n Lineal con preprocesamiento.\n+* VotingRegressor: combinaciÃ³n de `DecisionTreeRegressor`, `Ridge`, `Lasso` (voto promedio).\n+* Bagging (bootstrap) sobre Ã¡rboles de decisiÃ³n.\n+* Pasting (sin bootstrap) como contraste de muestreo.\n+* Gradient Boosting con bÃºsqueda de hiperparÃ¡metros vÃ­a `GridSearchCV` + `TimeSeriesSplit`.\n+* LightGBM (`LGBMRegressor`) con grid search y control de profundidad/hojas.\n+\n+Preprocesamiento y Features:\n+* Variables Ãºnicamente disponibles al momento del pickup para evitar leakage: `passenger_count`, `trip_distance`, `pickup_hour`, `pickup_dow`, `month`, `year`, `pu_location_id`, `service_type`, `vendor_id`, `rate_code_id`, `payment_type`.\n+* Limpieza: filtrado de outliers y reglas lÃ³gicas (rango de `total_amount`, `trip_distance`, duraciÃ³n, pasajeros).\n+* Capado de cardinalidad de `pu_location_id` (IDs > 265 agrupados).\n+* Split temporal fijo: Train (<=2022), ValidaciÃ³n (2023), Test (2024).\n+* Transformaciones: `StandardScaler` para numÃ©ricas y `OneHotEncoder(handle_unknown='ignore', max_categories=50)` para categÃ³ricas dentro de un `ColumnTransformer`.\n+* Se eliminaron polÃ­gonos/polimorfismo y generaciÃ³n polinomial para priorizar interpretabilidad y velocidad en ensembles.\n+\n+Muestreo y Estrategia de Carga:\n+* ExtracciÃ³n vÃ­a Spark JDBC desde `analytics.obt_trips` con query parametrizada y `random() <= 0.02` para generar una muestra balanceada multianual.\n+* Particionamiento por aÃ±o para lectura paralela y deduplicaciÃ³n antes de pasar a Pandas.\n+\n+Entrenamiento y BÃºsqueda de HiperparÃ¡metros:\n+* `TimeSeriesSplit(n_splits=5)` para respetar el orden temporal en Gradient Boosting y LightGBM.\n+* Grids concisos enfocÃ¡ndose en profundidad, tasa de aprendizaje, nÃºmero de estimadores y subsampling (`subsample`, `colsample_bytree`).\n+* Registro de tiempos de ajuste (segundos) para comparar costo computacional vs. mejora predictiva.\n+\n+EvaluaciÃ³n:\n+* MÃ©tricas: RMSE y MAE principales; RÂ² como referencia de varianza explicada.\n+* SelecciÃ³n del modelo final por menor RMSE en validaciÃ³n (2023).\n+* EvaluaciÃ³n final en Test (2024) sÃ³lo con el mejor pipeline para evitar sobre-reporting.\n+\n+Hallazgos Clave:\n+* LightGBM y Gradient Boosting ofrecen mejor trade-off entre error y estabilidad temporal.\n+* Bagging vs Pasting evidencian el impacto positivo del bootstrap bajo alta variabilidad de ubicaciones.\n+* VotingRegressor estabiliza pero no siempre supera a boosting cuando las relaciones no lineales dominan.\n+\n+PrÃ³ximos pasos potenciales (no implementados aÃºn):\n+* Stacking de nivel 2 (meta-modelo).\n+* Ajuste de tasa de muestreo dinÃ¡mica por aÃ±o para balances finos.\n+* Incorporar caracterÃ­sticas derivadas de distancia temporal (festivos, clima).\n+\n+Checklist actualizado aÃ±ade:\n+[x] Modelos Ensemble (Voting, Bagging, Gradient Boosting, LightGBM) comparados con baseline.\n+\n+Dependencias adicionales requeridas para esta fase (agregar en `requirements.txt` si se desea reproducibilidad directa): `scikit-learn`, `lightgbm`, `seaborn`, `matplotlib`, `python-dotenv`, `snowflake-connector-python`, `pyspark`.\n+\n+El modelo ganador se determina con base en el menor RMSE de validaciÃ³n manteniendo consistencia del MAE y sin degradar significativamente RÂ².\n+```
