<<<<<<< HEAD
```Universidad San Francisco de Quito
Data Mining
Proyecto 06
John Ochoa (00345743)

#Github Link:
https://github.com/johnnyredwood/PSET5-ENSEMBLE-REGRESSION

#Descripci√≥n del proyecto:
Infraestructura anal√≠tica completa con Docker Compose integrando Jupyter+Spark y Postgres para procesar el dataset NYC TLC Trips 2015‚Äì2025. Se ingesta cobertura Yellow y Green en esquema RAW y se construye una tabla anal√≠tica unificada (One Big Table) en el esquema `analytics` mediante un √∫nico comando (`build_obt.py`).

Sobre la OBT se realiza muestreo controlado y preparaci√≥n de features (scaling num√©ricas + one-hot categ√≥ricas) para entrenar y comparar modelos de regresi√≥n enfoc√°ndose en predecir `total_amount` al pickup. En esta versi√≥n (PSET5) se evoluciona desde modelos lineales regularizados hacia un set de modelos ensemble (Voting, Bagging, Pasting, Gradient Boosting y LightGBM) m√°s un baseline lineal, seleccionando el mejor por RMSE temporal en validaci√≥n y auditando desempe√±o en test.

#Checklist de aceptaci√≥n
[x] Docker Compose levanta Spark y Jupyter Notebook.
[x] Variables sensibles gestionadas v√≠a archivo .env.
[x] Cobertura 2015‚Äì2025 (Yellow/Green) cargada en RAW con monitoreo por lote.
[x] Tabla `analytics.obt_trips` construida con columnas base, derivadas y metadatos.
[x] Muestreo controlado y particionado temporal (Train ‚â§2022 / Val 2023 / Test 2024).
[x] Modelos Ensemble (Voting, Bagging, Pasting, Gradient Boosting, LightGBM) comparados con baseline.
[x] Selecci√≥n por menor RMSE en validaci√≥n manteniendo MAE y R¬≤ estables.
[x] README claro: pasos, variables, arquitectura, decisiones y troubleshooting.

#Variables de ambiente: listado y prop√≥sito; gu√≠a para .env.

Para la ejecuci√≥n de los notebooks se definieron las siguientes variables de ambiente de Snowflake:

SOURCE_PATH=https://d37ci6vzurychx.cloudfront.net
YEARS=2020,2021,2022,2023,2024,2025
MONTHS=1,2,3,4,5,6,7,8,9,10,11,12
SERVICES=yellow,green
JUPYTER_TOKEN=token123
PYSPARK_PYTHON=python3
SPARK_LOCAL_IP=0.0.0.0
PORT_JUPYTER=8888
PORT_SPARK=4040
PORT_POSTGRES=5432
PORT_WAREHOUSEUI=8080
POSTGRES_HOST=postgres
POSTGRES_DB=database123
POSTGRES_USER=usuario123
POSTGRES_PASSWORD=clave123
PGADMIN_DEFAULT_EMAIL=admin@admin.com
PGADMIN_DEFAULT_PASSWORD=adminPassword123
PG_USER=usuario_pg123
PG_DB=database123

Con estas variables siguiendo el ejemplo del .env.example incluido en el proyecto se puede reproducir el mismo con credenciales propias
de esta manera se gestiona correctamente los datos sensibles.

#Arquitectura (flujo resumido)

 Ingesta Parquet (Yellow/Green & Zones)
        ‚îÇ
        ‚ñº
 Esquema RAW (tablas particionadas + metadatos)
        ‚îÇ
        ‚ñº
 Construcci√≥n OBT (`analytics.obt_trips`)
        ‚îÇ
        ‚ñº
 Muestreo & Preparaci√≥n (limpieza, encoding, scaling)
        ‚îÇ
        ‚ñº
 Entrenamiento Modelos (Baseline + Ensembles)
        ‚îÇ
        ‚ñº
 Selecci√≥n & Evaluaci√≥n (Validaci√≥n / Test)


#Pasos para Docker Compose y ejecuci√≥n de notebooks (incluido comando para construir OBT).

Prerrequisitos
*Docker instalado
*Docker Compose instalado
*Archivo .env configurado con las credenciales de Snowflake

1. Descargar de repositorio y Configuraci√≥n del Ambiente

- Descargar el repositorio a su entorno local con
git clone https://github.com/johnnyredwood/PSET5-ENSEMBLE-REGRESSION/

Crear archivo de variables de ambiente:

- Copiar el template y configurar con valores reales
cp .env.example .env

- Editar el archivo .env con tus credenciales
nano .env

2. Verificar estructura de directorios (vista clave):

üìÅ drivers                  -> Dependencias externas (drivers JDBC, etc.)
üìÅ Evidencias               -> Capturas / artefactos de validaci√≥n y resultados
üìÅ init-scripts             -> SQL inicial (esquemas, permisos) para Postgres
‚îÇ   ‚îî‚îÄ‚îÄ 01-init-schemas.sql
üìÅ libros                   -> Notebooks de ingesta y modelado
‚îÇ   üìÅ .ipynb_checkpoints    -> Estados intermedios autom√°ticos
‚îÇ   ‚îú‚îÄ‚îÄ 01_ingesta_parquet_raw.ipynb  -> Ingesta masiva RAW
‚îÇ   ‚îú‚îÄ‚îÄ pset5_ensemble_regression.ipynb -> Entrenamiento y comparaci√≥n ensembles
‚îÇ   ‚îú‚îÄ‚îÄ checkpointTaxisGreen.json      -> Progreso ingesta Green
‚îÇ   ‚îú‚îÄ‚îÄ checkpointTaxisYellow.json     -> Progreso ingesta Yellow
‚îÇ   ‚îî‚îÄ‚îÄ postgresql-42.2.5.jar          -> Driver JDBC Postgres
üìÅ logs                     -> Logs operativos / seguimiento procesos
üìÅ scripts                  -> Scripts utilitarios (ETL / construcci√≥n OBT)
‚îÇ   ‚îî‚îÄ‚îÄ build_obt.py        -> Construcci√≥n tabla OBT parametrizada
üìÅ warehouse_data           -> Data directory Postgres (persistencia f√≠sica)
üìÅ warehouse_ui_data        -> Data de la UI (pgAdmin / sesiones)
‚îÇ   ‚îú‚îÄ‚îÄ azurecredentialcache
‚îÇ   ‚îú‚îÄ‚îÄ sessions
‚îÇ   ‚îú‚îÄ‚îÄ storage
‚îÇ   ‚îî‚îÄ‚îÄ pgadmin4.db
.env                        -> Variables de entorno locales (no versionar sensibles)
.env.example                -> Plantilla de referencia para reproducir entorno
.gitignore                  -> Exclusiones de control de versi√≥n
docker-compose.yaml         -> Orquestaci√≥n de servicios (Spark, Jupyter, Postgres, pgAdmin)
Dockerfile.obt-builder      -> Imagen especializada para construcci√≥n OBT
README.md                   -> Documentaci√≥n del proyecto
requirements.txt            -> Dependencias Python base

3. Inicializaci√≥n de la Infraestructura
Levantar los servicios con Docker Compose:

- Ejecutar en el directorio del proyecto el siguiente comando para levantar el contenedor con variables de entorno de .env
docker-compose --env-file .env up -d

- Verificar que el contenedor est√© corriendo
docker-compose ps

4. Acceder a Jupyter Notebook:
Acceder al Jupyter Notebook del contenedor con el puerto y token indicados en su .env

URL: http://localhost:puerto
Token: [valor de JUPYTER_TOKEN en .env]

5. Ejecuci√≥n Secuencial de Notebooks
Orden de ejecuci√≥n obligatorio:

Notebook 01 - 01_ingesta_parquet_raw
Par√°metros esperados:
- A√±os: 2015-2025 (configurado en .env)
- Meses: 1-12 (configurado en .env)  
- Servicios: yellow, green (configurado en .env)
Genera:
-Tabla RAW de datos de taxi por servicio en Snowflake

Construcci√≥n de tabla OBT

Teniendo los contenedores corriendo en Docker desde consola ejecutar el siguiente comando:

docker compose run obt-builder python /app/scripts/build_obt.py --year-start yearInicio --year-end yearFin --services serviciosSeparadosPorComa --run-id identificadorRun --months mesesSeparadosPorEspacio

De donde:
yearInicio es el a√±o en formato entero desde el cual se quieren empezar a procesar los datos (verificar disponibilidad de datos de dicho a√±o en esquema Raw)
yearFin es el a√±o en formato entero hasta el cual se quieren procesar los datos (verificar disponibilidad de datos de dicho a√±o en esquema Raw)
serviciosSeparadosPorComa son los servicios de los taxis en este caso aplica yellow,green
identificadorRun es un identificador que se poblara en la tabla obt se puede ingresar cualquiera que desee el usuario
mesesSeparadosPorEspacio son los meses de cada a√±o que se deseen procesar en formato entero separados por espacios

Ejemplos de dicho comando para ejecutar son:

docker compose run obt-builder python /app/scripts/build_obt.py --year-start 2020 --year-end 2020 --services yellow,green --run-id full_load --months 3 4 5 6 7 8 9 10 11 12

docker compose run obt-builder python /app/scripts/build_obt.py --year-start 2022 --year-end 2022 --services yellow,green --run-id full_load

#Dise√±o de raw y OBT (columnas, derivadas, metadatos, supuestos).

*Esquema RAW
El esquema raw funciona como capa de aterrizaje donde se preservan los datos en su 
formato original con metadatos de ingesta. Se implementaron tablas particionadas por servicio 
y per√≠odo para optimizar el manejo de los vol√∫menes de datos.

Estructura de tablas RAW:

NY_TAXI_RAW_YELLOW - Viajes de taxi amarillo RAW
NY_TAXI_RAW_GREEN - Viajes de taxi verde RAW
NY_TAXI_RAW_TAXI_ZONES - Zonas de Taxis de New York RAW

Columnas base preservadas del origen:

Datos temporales: pickup/dropoff datetime
Ubicaciones: PULocationID, DOLocationID
M√©tricas de viaje: trip_distance, passenger_count
Tarifas: fare_amount, tip_amount, tolls_amount, total_amount
Identificadores: VendorID, RatecodeID, payment_type

Metadatos de ingesta agregados:

run_id - Identificador √∫nico de la ejecuci√≥n
source_year / source_month - Per√≠odo de origen
ingested_at_utc - Timestamp de ingesta
service_type - Tipo de servicio (yellow/green)

*Esquema ANALYTICS - OBT
La OBT consolida todos los datos de viajes de taxis de New York en una tabla que junta toda 
la informaci√≥n necesaria validada y depurada a manera de ejecutar consultas de negocio
sobre la misma

Columnas de la OBT:

Temporales:
pickup_datetime, dropoff_datetime - Timestamps originales
pickup_date, pickup_hour - Componentes temporales
dropoff_date, dropoff_hour - Componentes temporales
trip_duration_min - Duraci√≥n calculada en minutos

Ubicaciones:
pu_location_id, do_location_id - IDs originales
pu_zone, pu_borough - Nombres desnormalizados
do_zone, do_borough - Nombres desnormalizados

Servicio y C√≥digos:
vendor_id, vendor_name - Desnormalizado
rate_code_id, rate_code_desc - Desnormalizado
payment_type, payment_type_desc - Desnormalizado

M√©tricas y Tarifas:
passenger_count, trip_distance
fare_amount, extra, mta_tax, tip_amount
tolls_amount, improvement_surcharge
congestion_surcharge, airport_fee, total_amount

Metadatos:
run_id - Trazabilidad de la ejecuci√≥n
ingested_at_utc - Fecha de procesamiento
source_service - Servicio de origen
source_year, source_month - Per√≠odo origen

Supuestos de Dise√±o
Clave Natural: Se define basada en pickup_datetime, PULocationID, DOLocationID y VendorID para garantizar identificaci√≥n √∫nica de viajes en merges

Estrategia de Idempotencia: Implementaci√≥n de UPSERT basado en clave natural, permitiendo reingesta sin duplicados.

Manejo de Datos: Se han filtrado nulos en campos obligatorios y se ha definido validaciones l√≥gicas para datos n√∫mericos de forma que los mismos
cumplan con rangos l√≥gicos

#Calidad/auditor√≠a: qu√© se valida y d√≥nde se ve.

*Validaci√≥n de Conectividad con Snowflake desde Spark:
Inicio sesi√≥n de Spark y posteriormente genero una conexi√≥n con Snowflake con mis credenciales y ejecuto una query simple de SELECT current_version()
esto lo valido en todos los notebooks antes de proceder con el consumo, procesamiento y/o lectura de datos

*Validaci√≥n de Ingesta de datos:
En todos los notebooks he implementado logs en forma de prints y manejo de excepciones para ir monitoreando el proceso de consumo de todos los datos.
A su vez una vez los mismos se iban consumiendo ingresaba en Snowflake a verificar que las tablas aumenten en cantidad de filas y monitoreaba los datos
recien ingresados con queries simples desde Snowflake

*Validaci√≥n del contenedor de docker:
Al tener spark-notebook: Jupyter+Spark desde un contenedor de docker verificaba que el mismo estuviera funcionando correctamente con el comando docker ps,
con el Docker Desktop verificando que el contenedor este arriba e ingresando a localhost con el puerto definido y verificando que pudiera ingresar
sin problema a Jupyter

*Comentarios respecto a modelos ML:

Enfoque actualizado (PSET5 - Modelos Ensemble)

Se migr√≥ del enfoque de modelos lineales regularizados (SGD, Ridge, Lasso, ElasticNet desde cero y sklearn) hacia un conjunto de modelos ensemble y comparativos para mejorar capacidad predictiva sobre total_amount y robustez temporal.

Modelos incluidos:

Baseline: Regresi√≥n Lineal con preprocesamiento.

VotingRegressor: combinaci√≥n de DecisionTreeRegressor, Ridge, Lasso (voto promedio).

Bagging: bootstrap sobre √°rboles de decisi√≥n.

Pasting: muestreo sin bootstrap como contraste.

Gradient Boosting: con b√∫squeda de hiperpar√°metros v√≠a GridSearchCV + TimeSeriesSplit.

LightGBM (LGBMRegressor) con grid search y control de profundidad/hojas.

Preprocesamiento y Features

Variables disponibles solo al momento del pickup para evitar leakage:
passenger_count, trip_distance, pickup_hour, pickup_dow, month, year, pu_location_id, service_type, vendor_id, rate_code_id, payment_type.

Limpieza: filtrado de outliers y reglas l√≥gicas (rango de total_amount, trip_distance, duraci√≥n, pasajeros).

Capado de cardinalidad de pu_location_id (IDs > 265 agrupados).

Split temporal fijo:

Train (2022)

Validaci√≥n (2023)

Test (2024)

Transformaciones:

StandardScaler para num√©ricas

OneHotEncoder(handle_unknown='ignore', max_categories=50) para categ√≥ricas
Todo dentro de un ColumnTransformer.

Se eliminaron polinomios y generaci√≥n polinomial para priorizar interpretabilidad y velocidad en ensembles.

Muestreo y Estrategia de Carga

Extracci√≥n v√≠a Spark JDBC desde analytics.obt_trips con query parametrizada y random() <= 0.02 para generar una muestra balanceada multianual.

Particionamiento por a√±o para lectura paralela y deduplicaci√≥n antes de pasar a Pandas.

Entrenamiento y B√∫squeda de Hiperpar√°metros

TimeSeriesSplit(n_splits=5) para respetar el orden temporal en Gradient Boosting y LightGBM.

Grids concisos enfocados en profundidad, tasa de aprendizaje, n√∫mero de estimadores y subsampling (subsample, colsample_bytree).

Registro de tiempos de ajuste (segundos) para comparar costo computacional vs. mejora predictiva.

Evaluaci√≥n

M√©tricas principales: RMSE y MAE.

R¬≤ como referencia de varianza explicada.

Modelo final elegido por menor RMSE en validaci√≥n (2023).

Evaluaci√≥n final en Test (2024) usando solo el mejor pipeline para evitar over-reporting.

Hallazgos Clave

LightGBM y Gradient Boosting ofrecen mejor trade-off entre error y estabilidad temporal.

Bagging vs Pasting evidencia el impacto positivo del bootstrap bajo alta variabilidad de ubicaciones.

VotingRegressor estabiliza el error pero no siempre supera a boosting cuando dominan relaciones no lineales.

Pr√≥ximos pasos potenciales (pendientes)

Stacking de nivel 2 (meta-modelo).

Ajuste de tasa de muestreo din√°mica por a√±o para balances finos.

Incorporar caracter√≠sticas derivadas de distancia temporal (festivos, clima).

Checklist actualizado

Modelos Ensemble (Voting, Bagging, Gradient Boosting, LightGBM) comparados con baseline.

Notas finales

El modelo ganador se determina con base en el menor RMSE de validaci√≥n, manteniendo consistencia del MAE y sin degradar significativamente R¬≤.```
=======
# Proyecto-ActivoFinanciero
Proyecto de Activo Financiero- Data Mining
>>>>>>> 4286953649b8d2f2579eb0a6041828ff39f9c067
