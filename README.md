```Universidad San Francisco de Quito
Data Mining
Proyecto 05

#Github Link:
[https://github.com/johnnyredwood/PSET4_NYTAXIS_ML_SCRATCH_SCIKIT](https://github.com/johnnyredwood/PSET5-ENSEMBLE-REGRESSION)

#Descripci√≥n del proyecto:
Implement√© una infraestructura anal√≠tica completa utilizando Docker Compose que integra Jupyter+Spark con Postgres para procesar el dataset 
NYC TLC Trips 2015-2025. El proyecto replica el proceso de ingesta de datos Parquet de taxis Yellow y Green hacia un esquema raw en Snowflake, 
construyendo posteriormente una tabla anal√≠tica unificada (One Big Table) en el esquema analytics con un √∫nico comando a trav√©s del uso de un script
de Python

Con los datos en analytics genere la limpieza y preparaci√≥n de los mismos para posterior aplicaci√≥n de algoritmos de ML orientados a predecir el total_amount
de pago para el pickup acorde a variables dadas por los datos. Estas predicciones se basan en modelos from Scratch y de Scikit Learn de Stochastic Gradient Descent,
Lasso, Ridge y ElasticNet afinados para los mejores par√°metros a trav√©s de Grid Search. Una vez generados los modelos se genero una comparaci√≥n de los mismos 
a manera de seleccionar el mejor modelo para la predicci√≥n adecuada del precio.

#Checklist de aceptaci√≥n
[x] Docker Compose levanta Spark y Jupyter Notebook.
[x] Todas las credenciales/par√°metros provienen de variables de ambiente (.env).
[x] Cobertura 2015‚Äì2025 (Yellow/Green) cargada en raw con matriz y conteos por lote.
[x] analytics.obt_trips creada con columnas m√≠nimas, derivadas y metadatos.
[x] Modelos ML Scratch vs ScikitLearn
[x] README claro: pasos, variables, esquema, decisiones, troubleshooting.

#Variables de ambiente: listado y prop√≥sito; gu√≠a para .env.

Para la ejecuci√≥n de los notebooks se definieron las siguientes variables de ambiente de Snowflake:

SOURCE_PATH=https://d37ci6vzurychx.cloudfront.net
YEARS=2020,2021,2022,2023,2024,2025
MONTHS=1,2,3,4,5,6,7,8,9,10,11,12
SERVICES=yellow,green
JUPYTER_TOKEN=token123
PYSPARK_PYTHON=python3
SPARK_LOCAL_IP=0.0.0.0
PORT_JUPYTER=8888
PORT_SPARK=4040
PORT_POSTGRES=5432
PORT_WAREHOUSEUI=8080
POSTGRES_HOST=postgres
POSTGRES_DB=database123
POSTGRES_USER=usuario123
POSTGRES_PASSWORD=clave123
PGADMIN_DEFAULT_EMAIL=admin@admin.com
PGADMIN_DEFAULT_PASSWORD=adminPassword123
PG_USER=usuario_pg123
PG_DB=database123

Con estas variables siguiendo el ejemplo del .env.example incluido en el proyecto se puede reproducir el mismo con credenciales propias
de esta manera se gestiona correctamente los datos sensibles.

#Arquitectura (diagrama/tabla): Spark/Jupyter ‚Üí Snowflake (raw ‚Üí analytics.obt_trips).

 Loaders Raw - Taxi Trips y Zones
        ‚îÇ
        ‚ñº
 Construcci√≥n OBT- Analytics
        ‚îÇ
        ‚ñº
 Preparaci√≥n de Datos
        ‚îÇ
        ‚ñº
 Modelos ML Scratch
        ‚îÇ
        ‚ñº
 Modelos ML Scikit Learn
        ‚îÇ
        ‚ñº
 Comparaci√≥n Modelos


#Pasos para Docker Compose y ejecuci√≥n de notebooks (incluido comando para construir OBT).

Prerrequisitos
*Docker instalado
*Docker Compose instalado
*Archivo .env configurado con las credenciales de Snowflake

1. Descargar de repositorio y Configuraci√≥n del Ambiente

- Descargar el repositorio a su entorno local con
git clone https://github.com/johnnyredwood/PSET4_NYTAXIS_ML_SCRATCH_SCIKIT

Crear archivo de variables de ambiente:

- Copiar el template y configurar con valores reales
cp .env.example .env

- Editar el archivo .env con tus credenciales
nano .env

2. Verificar estructura de directorios:

üìÅ drivers
üìÅ Evidencias
üìÅ init-scripts
‚îÇ   ‚îî‚îÄ‚îÄ 01-init-schemas.sql
üìÅ libros
‚îÇ   üìÅ .ipynb_checkpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01_ingesta_parquet_raw-checkpoint.ipynb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ checkpointTaxisYellow-checkpoint.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ml_total_amount_regression-checkpoint.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 01_ingesta_parquet_raw.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ checkpointTaxisGreen.json
‚îÇ   ‚îú‚îÄ‚îÄ checkpointTaxisYellow.json
‚îÇ   ‚îú‚îÄ‚îÄ ml_total_amount_regression.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ postgresql-42.2.5.jar
üìÅ logs
üìÅ scripts
‚îÇ   ‚îî‚îÄ‚îÄ build_obt.py
üìÅ warehouse_data
üìÅ warehouse_ui_data
‚îÇ   ‚îú‚îÄ‚îÄ azurecredentialcache
‚îÇ   ‚îú‚îÄ‚îÄ sessions
‚îÇ   ‚îú‚îÄ‚îÄ storage
‚îÇ   ‚îî‚îÄ‚îÄ pgadmin4.db
.env
.env.example
.gitignore
docker-compose.yaml
Dockerfile.obt-builder
README.md
requirements.txt

3. Inicializaci√≥n de la Infraestructura
Levantar los servicios con Docker Compose:

- Ejecutar en el directorio del proyecto el siguiente comando para levantar el contenedor con variables de entorno de .env
docker-compose --env-file .env up -d

- Verificar que el contenedor est√© corriendo
docker-compose ps

4. Acceder a Jupyter Notebook:
Acceder al Jupyter Notebook del contenedor con el puerto y token indicados en su .env

URL: http://localhost:puerto
Token: [valor de JUPYTER_TOKEN en .env]

5. Ejecuci√≥n Secuencial de Notebooks
Orden de ejecuci√≥n obligatorio:

Notebook 01 - 01_ingesta_parquet_raw
Par√°metros esperados:
- A√±os: 2015-2025 (configurado en .env)
- Meses: 1-12 (configurado en .env)  
- Servicios: yellow, green (configurado en .env)
Genera:
-Tabla RAW de datos de taxi por servicio en Snowflake

Construcci√≥n de tabla OBT

Teniendo los contenedores corriendo en Docker desde consola ejecutar el siguiente comando:

docker compose run obt-builder python /app/scripts/build_obt.py --year-start yearInicio --year-end yearFin --services serviciosSeparadosPorComa --run-id identificadorRun --months mesesSeparadosPorEspacio

De donde:
yearInicio es el a√±o en formato entero desde el cual se quieren empezar a procesar los datos (verificar disponibilidad de datos de dicho a√±o en esquema Raw)
yearFin es el a√±o en formato entero hasta el cual se quieren procesar los datos (verificar disponibilidad de datos de dicho a√±o en esquema Raw)
serviciosSeparadosPorComa son los servicios de los taxis en este caso aplica yellow,green
identificadorRun es un identificador que se poblara en la tabla obt se puede ingresar cualquiera que desee el usuario
mesesSeparadosPorEspacio son los meses de cada a√±o que se deseen procesar en formato entero separados por espacios

Ejemplos de dicho comando para ejecutar son:

docker compose run obt-builder python /app/scripts/build_obt.py --year-start 2020 --year-end 2020 --services yellow,green --run-id full_load --months 3 4 5 6 7 8 9 10 11 12

docker compose run obt-builder python /app/scripts/build_obt.py --year-start 2022 --year-end 2022 --services yellow,green --run-id full_load

#Dise√±o de raw y OBT (columnas, derivadas, metadatos, supuestos).

*Esquema RAW
El esquema raw funciona como capa de aterrizaje donde se preservan los datos en su 
formato original con metadatos de ingesta. Se implementaron tablas particionadas por servicio 
y per√≠odo para optimizar el manejo de los vol√∫menes de datos.

Estructura de tablas RAW:

NY_TAXI_RAW_YELLOW - Viajes de taxi amarillo RAW
NY_TAXI_RAW_GREEN - Viajes de taxi verde RAW
NY_TAXI_RAW_TAXI_ZONES - Zonas de Taxis de New York RAW

Columnas base preservadas del origen:

Datos temporales: pickup/dropoff datetime
Ubicaciones: PULocationID, DOLocationID
M√©tricas de viaje: trip_distance, passenger_count
Tarifas: fare_amount, tip_amount, tolls_amount, total_amount
Identificadores: VendorID, RatecodeID, payment_type

Metadatos de ingesta agregados:

run_id - Identificador √∫nico de la ejecuci√≥n
source_year / source_month - Per√≠odo de origen
ingested_at_utc - Timestamp de ingesta
service_type - Tipo de servicio (yellow/green)

*Esquema ANALYTICS - OBT
La OBT consolida todos los datos de viajes de taxis de New York en una tabla que junta toda 
la informaci√≥n necesaria validada y depurada a manera de ejecutar consultas de negocio
sobre la misma

Columnas de la OBT:

Temporales:
pickup_datetime, dropoff_datetime - Timestamps originales
pickup_date, pickup_hour - Componentes temporales
dropoff_date, dropoff_hour - Componentes temporales
trip_duration_min - Duraci√≥n calculada en minutos

Ubicaciones:
pu_location_id, do_location_id - IDs originales
pu_zone, pu_borough - Nombres desnormalizados
do_zone, do_borough - Nombres desnormalizados

Servicio y C√≥digos:
vendor_id, vendor_name - Desnormalizado
rate_code_id, rate_code_desc - Desnormalizado
payment_type, payment_type_desc - Desnormalizado

M√©tricas y Tarifas:
passenger_count, trip_distance
fare_amount, extra, mta_tax, tip_amount
tolls_amount, improvement_surcharge
congestion_surcharge, airport_fee, total_amount

Metadatos:
run_id - Trazabilidad de la ejecuci√≥n
ingested_at_utc - Fecha de procesamiento
source_service - Servicio de origen
source_year, source_month - Per√≠odo origen

Supuestos de Dise√±o
Clave Natural: Se define basada en pickup_datetime, PULocationID, DOLocationID y VendorID para garantizar identificaci√≥n √∫nica de viajes en merges

Estrategia de Idempotencia: Implementaci√≥n de UPSERT basado en clave natural, permitiendo reingesta sin duplicados.

Manejo de Datos: Se han filtrado nulos en campos obligatorios y se ha definido validaciones l√≥gicas para datos n√∫mericos de forma que los mismos
cumplan con rangos l√≥gicos

#Calidad/auditor√≠a: qu√© se valida y d√≥nde se ve.

*Validaci√≥n de Conectividad con Snowflake desde Spark:
Inicio sesi√≥n de Spark y posteriormente genero una conexi√≥n con Snowflake con mis credenciales y ejecuto una query simple de SELECT current_version()
esto lo valido en todos los notebooks antes de proceder con el consumo, procesamiento y/o lectura de datos

*Validaci√≥n de Ingesta de datos:
En todos los notebooks he implementado logs en forma de prints y manejo de excepciones para ir monitoreando el proceso de consumo de todos los datos.
A su vez una vez los mismos se iban consumiendo ingresaba en Snowflake a verificar que las tablas aumenten en cantidad de filas y monitoreaba los datos
recien ingresados con queries simples desde Snowflake

*Validaci√≥n del contenedor de docker:
Al tener spark-notebook: Jupyter+Spark desde un contenedor de docker verificaba que el mismo estuviera funcionando correctamente con el comando docker ps,
con el Docker Desktop verificando que el contenedor este arriba e ingresando a localhost con el puerto definido y verificando que pudiera ingresar
sin problema a Jupyter

*Comentarios respecto a modelos ML:

Enfoque general

Se implementaron cuatro modelos lineales regularizados desde cero (SGD, Ridge, Lasso, Elastic Net) utilizando NumPy puro, para demostrar comprensi√≥n de los algoritmos de optimizaci√≥n y regularizaci√≥n.

Cada modelo tiene su versi√≥n equivalente en scikit learn con id√©ntico preprocesamiento, lo que permite una comparaci√≥n justa y reproducible de rendimiento y tiempo.

2. Preprocesamiento y features

Se incluyeron √∫nicamente variables disponibles en pickup para evitar data leakage.

El pipeline com√∫n incluy√≥:

Imputaci√≥n de valores ausentes.

Escalado (StandardScaler) obligatorio para los modelos con penalizaci√≥n L1/L2.

Codificaci√≥n One-Hot de variables categ√≥ricas controlando cardinalidad (Top-K + ‚ÄúOther‚Äù).

PolynomialFeatures en variables num√©ricas clave (trip_distance, pickup_hour, passenger_count) para capturar interacciones no lineales.

Se mantuvieron seeds fijas y un split temporal (Train: a√±os antiguos, Valid: intermedios, Test: recientes) para garantizar comparabilidad y reproducibilidad.

3. Modelos from-scratch

SGD implementado con descenso de gradiente estoc√°stico y tasa de aprendizaje adaptable.

Ridge, Lasso y Elastic Net resolvieron sus penalizaciones mediante optimizaci√≥n iterativa tipo coordinate descent o gradiente regularizado.

Cada modelo se encapsul√≥ con m√©todos .fit() y .predict() y m√©tricas internas de convergencia.

4. Comparaci√≥n con scikit-learn

Los pipelines equivalentes (SGDRegressor, Ridge, Lasso, ElasticNet) de scikit-learn se configuraron con los mismos hiperpar√°metros (alpha, l1_ratio, eta0, max_iter), escalador, polinomios

Se realiz√≥ b√∫squeda en malla (GridSearch) comparable entre ambas versiones, registrando m√©tricas (RMSE, MAE, R cuadrado) y tiempos.

Las implementaciones propias mostraron resultados coherentes con sklearn, validando la correcta implementaci√≥n matem√°tica de los modelos.

5. Evaluaci√≥n y m√©tricas

M√©tricas utilizadas: RMSE y MAE como principales; R cuadrado como secundaria.

Se report√≥ una tabla comparativa completa con los ocho pipelines (4 propios + 4 sklearn) y an√°lisis de estabilidad frente a alpha y l1_ratio.

El modelo ganador se seleccion√≥ con base en menor RMSE en validaci√≥n
