{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e29b0efe-78c5-45f3-97cc-9271ed2de92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (1.2.1)\n",
      "Requirement already satisfied: snowflake-connector-python in /opt/conda/lib/python3.11/site-packages (4.0.0)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.5.1)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (41.0.4)\n",
      "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (23.2.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.8.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2023.3.post1)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.31.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (23.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions<5,>=4.3 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (4.8.0)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.20.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.11.0)\n",
      "Requirement already satisfied: tomlkit in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (0.13.3)\n",
      "Requirement already satisfied: boto3>=1.24 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.40.75)\n",
      "Requirement already satisfied: botocore>=1.24 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.40.75)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3>=1.24->snowflake-connector-python) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /opt/conda/lib/python3.11/site-packages (from boto3>=1.24->snowflake-connector-python) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore>=1.24->snowflake-connector-python) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore>=1.24->snowflake-connector-python) (2.0.7)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.11/site-packages (from cryptography>=3.1.0->snowflake-connector-python) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=3.1.0->snowflake-connector-python) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python) (1.16.0)\n",
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.11-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
      "Downloading psycopg2_binary-2.9.11-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.11\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install snowflake-connector-python\n",
    "!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acee23d5-1b8a-4216-bcac-c7e3a520ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d549fe34-75ea-49af-b7d5-057d00224ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-03 01:31:06--  https://jdbc.postgresql.org/download/postgresql-42.2.5.jar\n",
      "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
      "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 825943 (807K) [application/java-archive]\n",
      "Saving to: ‘./postgresql-42.2.5.jar.3’\n",
      "\n",
      "postgresql-42.2.5.j 100%[===================>] 806.58K  1.08MB/s    in 0.7s    \n",
      "\n",
      "2025-11-03 01:31:07 (1.08 MB/s) - ‘./postgresql-42.2.5.jar.3’ saved [825943/825943]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://jdbc.postgresql.org/download/postgresql-42.2.5.jar -P ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ac03bfc-da81-4664-96cb-2dea96a5db9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAR file exists: True\n",
      "JAR file size: 825943 bytes\n",
      "PORT_POSTGRES: 5432\n",
      "POSTGRES_DB: ny_taxi\n",
      "POSTGRES_USER: usuario_spark\n",
      "POSTGRES_PASSWORD set: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "jar_path = \"/home/jovyan/work/postgresql-42.2.5.jar\"\n",
    "print(f\"JAR file exists: {os.path.exists(jar_path)}\")\n",
    "print(f\"JAR file size: {os.path.getsize(jar_path) if os.path.exists(jar_path) else 'N/A'} bytes\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(f\"PORT_POSTGRES: {os.getenv('PORT_POSTGRES')}\")\n",
    "print(f\"POSTGRES_DB: {os.getenv('POSTGRES_DB')}\")\n",
    "print(f\"POSTGRES_USER: {os.getenv('POSTGRES_USER')}\")\n",
    "print(f\"POSTGRES_PASSWORD set: {bool(os.getenv('POSTGRES_PASSWORD'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e51ec312-09a1-4bad-8490-84f7f6e77c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql import Row \n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.jars\", \"/home/jovyan/work/postgresql-42.2.5.jar\").master(\"local\").appName(\"PySpark_Postgres_test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c04f87aa-eec4-4537-89ed-675318364229",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://warehouses:5432/{os.getenv('POSTGRES_DB')}\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"raw.taxi_zone_lookup\") \\\n",
    "    .option(\"user\", os.getenv('POSTGRES_USER')) \\\n",
    "    .option(\"password\", os.getenv('POSTGRES_PASSWORD')) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aa1936c-4fe2-4004-8b80-e60c9f844ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- locationid: integer (nullable = true)\n",
      " |-- borough: string (nullable = true)\n",
      " |-- zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      " |-- ingested_at_utc: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aca0aa0f-be73-492c-a04a-0a216bcf328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingestar_zones_a_raw():\n",
    "    SOURCE_PATH = os.getenv(\"SOURCE_PATH\")\n",
    "    path_url = f\"{SOURCE_PATH}/misc/taxi_zone_lookup.csv\"\n",
    "    local_path = f\"/tmp/taxiZones.parquet\"\n",
    "    \n",
    "    # Descargo el archivo en carpeta temporal para posteriormente leerlo\n",
    "    try:\n",
    "        r = requests.get(path_url, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            with open(local_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=10000000):\n",
    "                    f.write(chunk)\n",
    "        else:\n",
    "            print(f\"Archivo no encontrado en {path_url} (status {r.status_code})\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error descargando {path_url}: {e}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Archivo obtenido exitosamente de: {path_url}\")\n",
    "    \n",
    "    # Leo el archivo en un df de Spark\n",
    "    try:\n",
    "        df = spark.read.csv(local_path, header=\"true\")\n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo leer {local_path}: {e}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Archivo leido exitosamente por Spark: {local_path}\")\n",
    "\n",
    "    conteoFilas = df.count()\n",
    "    print(f\"Ingestando hacia Snowflake datos de Zonas de Taxis. Total de filas: {conteoFilas}\")\n",
    "\n",
    "    try:\n",
    "        df.write.format(\"jdbc\") \\\n",
    "            .option(\"url\", f\"jdbc:postgresql://warehouses:5432/{os.getenv('POSTGRES_DB')}\") \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .option(\"dbtable\", \"raw.taxi_zone_lookup\") \\\n",
    "            .option(\"user\", os.getenv('POSTGRES_USER')) \\\n",
    "            .option(\"password\", os.getenv('POSTGRES_PASSWORD')) \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "    except Exception as e2:\n",
    "        print(f\"Error con ingreso de datos: {e2}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(\"Zonas de taxis exportadas correctamente a Raw de Snowflake\")\n",
    "\n",
    "    #Me aseguro de eliminar el archivo parquet temporal\n",
    "    try:\n",
    "        os.remove(local_path)\n",
    "        print(f\"Archivo parquet temporal removido: {local_path}\")\n",
    "    except OSError as e:\n",
    "        print(f\"No se pudo remover el archivo parquet temporal {local_path}: {e}\")\n",
    "\n",
    "    #Retorno datos para tabla de conteos de datos consumidos por run\n",
    "    return {\n",
    "        \"count\": conteoFilas,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5baf7d12-66d1-4ca2-95f5-3c1b60011838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
      "Archivo leido exitosamente por Spark: /tmp/taxiZones.parquet\n",
      "Ingestando hacia Snowflake datos de Zonas de Taxis. Total de filas: 265\n",
      "Zonas de taxis exportadas correctamente a Raw de Snowflake\n",
      "Archivo parquet temporal removido: /tmp/taxiZones.parquet\n",
      "{'count': 265}\n"
     ]
    }
   ],
   "source": [
    "#Cargo Zonas de Taxis en Snowflake\n",
    "zonasTaxisIngesta=ingestar_zones_a_raw()\n",
    "print(zonasTaxisIngesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4490de1-45aa-48b9-8744-67fb80c27515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "#Hago la presente funcion para generar un identificador unico asociado a cada carga de datos para el RUN_ID \n",
    "def generar_run_id():\n",
    "    return datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b416ba18-280c-4e26-b4de-7a64f2f2ed42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla yellow y green creada correctamente en Postgres.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=\"warehouses\",\n",
    "    port=5432,\n",
    "    database=os.getenv(\"POSTGRES_DB\"),\n",
    "    user=os.getenv(\"POSTGRES_USER\"),\n",
    "    password=os.getenv(\"POSTGRES_PASSWORD\")\n",
    ")\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "sql_yellow = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS raw.yellow_taxi_trip (\n",
    "    vendorid INT,\n",
    "    tpep_pickup_datetime TIMESTAMP,\n",
    "    tpep_dropoff_datetime TIMESTAMP,\n",
    "    passenger_count FLOAT,\n",
    "    trip_distance FLOAT,\n",
    "    ratecodeid INT,\n",
    "    store_and_fwd_flag VARCHAR,\n",
    "    pulocationid INT,\n",
    "    dolocationid INT,\n",
    "    payment_type INT,\n",
    "    fare_amount FLOAT,\n",
    "    extra FLOAT,\n",
    "    mta_tax FLOAT,\n",
    "    tip_amount FLOAT,\n",
    "    tolls_amount FLOAT,\n",
    "    improvement_surcharge FLOAT,\n",
    "    total_amount FLOAT,\n",
    "    congestion_surcharge FLOAT,\n",
    "    airport_fee FLOAT,\n",
    "    run_id VARCHAR,\n",
    "    service_type VARCHAR,\n",
    "    source_year INT,\n",
    "    source_month INT,\n",
    "    ingested_at_utc TIMESTAMPTZ,\n",
    "    source_path VARCHAR\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "sql_green = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS raw.green_taxi_trip (\n",
    "    vendorid INT,\n",
    "    lpep_pickup_datetime TIMESTAMP,\n",
    "    lpep_dropoff_datetime TIMESTAMP,\n",
    "    passenger_count FLOAT,\n",
    "    trip_distance FLOAT,\n",
    "    ratecodeid INT,\n",
    "    store_and_fwd_flag VARCHAR,\n",
    "    pulocationid INT,\n",
    "    dolocationid INT,\n",
    "    payment_type INT,\n",
    "    fare_amount FLOAT,\n",
    "    extra FLOAT,\n",
    "    mta_tax FLOAT,\n",
    "    tip_amount FLOAT,\n",
    "    tolls_amount FLOAT,\n",
    "    improvement_surcharge FLOAT,\n",
    "    total_amount FLOAT,\n",
    "    congestion_surcharge FLOAT,\n",
    "    airport_fee FLOAT,\n",
    "    run_id VARCHAR,\n",
    "    service_type VARCHAR,\n",
    "    source_year INT,\n",
    "    source_month INT,\n",
    "    ingested_at_utc TIMESTAMPTZ,\n",
    "    source_path VARCHAR\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "cur.execute(sql_yellow)\n",
    "cur.execute(sql_green)\n",
    "conn.commit()\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Tabla yellow y green creada correctamente en Postgres.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1267211c-a36a-46b8-95fd-c3d84e0911d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests \n",
    "from pyspark.sql.functions import lit, current_timestamp \n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "def ingestar_parquet_a_raw(service: str, year: int, month: int):\n",
    "    SOURCE_PATH = os.getenv(\"SOURCE_PATH\")\n",
    "    path_url = f\"{SOURCE_PATH}/trip-data/{service}_tripdata_{year}-{month:02d}.parquet\"\n",
    "    local_path = f\"/tmp/{service}_tripdata_{year}-{month:02d}.parquet\"\n",
    "    print(path_url)\n",
    "    \n",
    "    # Descargo el archivo Parquet en carpeta temporal para posteriormente leerlo\n",
    "    try:\n",
    "        r = requests.get(path_url, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            with open(local_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=10000000):\n",
    "                    f.write(chunk)\n",
    "        else:\n",
    "            print(f\"Archivo no encontrado en {path_url} (status {r.status_code})\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error descargando {path_url}: {e}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Archivo obtenido exitosamente de: {path_url}\")\n",
    "    \n",
    "    # Leo el archivo parquet en un df de Spark\n",
    "    try:\n",
    "        df = spark.read.parquet(local_path)\n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo leer {local_path}: {e}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Archivo leido exitosamente por Spark: {local_path}\")\n",
    "\n",
    "    run_id = generar_run_id()\n",
    "\n",
    "    # Elimino columna conflictiva que solo esta presente en unos pocos parquets\n",
    "    cols_to_drop = [\"cbd_congestion_fee\", \"ehail_fee\", \"trip_type\"]\n",
    "\n",
    "    for c in cols_to_drop:\n",
    "        if c in df.columns:\n",
    "            df = df.drop(c)\n",
    "\n",
    "    # Añado los metadatos indicados en instrucciones de PSET\n",
    "    df_meta = df.withColumn(\"run_id\", lit(run_id)) \\\n",
    "                .withColumn(\"service_type\", lit(service)) \\\n",
    "                .withColumn(\"source_year\", lit(year)) \\\n",
    "                .withColumn(\"source_month\", lit(month)) \\\n",
    "                .withColumn(\"ingested_at_utc\", current_timestamp()) \\\n",
    "                .withColumn(\"source_path\", lit(path_url))\n",
    "\n",
    "    # Convierto tipos de fecha a Timestamp porque me estaba marcando error al enviar datos al Snowflake sin esta transformacion\n",
    "    for field in df_meta.schema.fields:\n",
    "        if field.dataType.typeName() == \"timestamp_ntz\":\n",
    "            df_meta = df_meta.withColumn(field.name, df_meta[field.name].cast(TimestampType()))\n",
    "\n",
    "    primary_keys = [\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\",\"PULocationID\",\"DoLocationID\",\"VendorID\"]\n",
    "\n",
    "    if (service==\"green\"):\n",
    "        primary_keys = [\"lpep_pickup_datetime\",\"lpep_dropoff_datetime\",\"PULocationID\",\"DoLocationID\",\"VendorID\"]\n",
    "\n",
    "    df_meta = df_meta.dropDuplicates(primary_keys)\n",
    "\n",
    "    conteoFilas = df_meta.count()\n",
    "    print(f\"Ingestando hacia Snowflake {service} {year}-{month}. Total de filas: {conteoFilas}\")\n",
    "\n",
    "    try:\n",
    "        df_meta.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", f\"jdbc:postgresql://warehouses:5432/{os.getenv('POSTGRES_DB')}\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .option(\"dbtable\", f\"raw.{service}_taxi_trip\") \\\n",
    "        .option(\"user\", os.getenv('POSTGRES_USER')) \\\n",
    "        .option(\"password\", os.getenv('POSTGRES_PASSWORD')) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "    except Exception as e2:\n",
    "        print(f\"Error con tabla temporal: {e2}\")\n",
    "        return None\n",
    "\n",
    "    #Me aseguro de eliminar el archivo parquet temporal\n",
    "    try:\n",
    "        os.remove(local_path)\n",
    "        print(f\"Archivo parquet temporal removido: {local_path}\")\n",
    "    except OSError as e:\n",
    "        print(f\"No se pudo remover el archivo parquet temporal {local_path}: {e}\")\n",
    "\n",
    "    #Retorno datos para tabla de conteos de datos consumidos por run\n",
    "    return {\n",
    "        \"year\": year,\n",
    "        \"month\": month,\n",
    "        \"count\": conteoFilas,\n",
    "        \"run_id\": run_id,\n",
    "        \"service_type\": service\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77b8bdfd-f83d-41a8-9111-1f457512bdff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpointTaxisYellow.json\n",
      "checkpoint: {'year': 2024, 'month': 12}\n",
      "checkpointTaxisGreen.json\n",
      "checkpoint: {'year': 0, 'month': 0}\n",
      "Iniciando ingesta de datos de taxis green: 1-2022\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2022-01.parquet\n",
      "Ingestando hacia Snowflake green 2022-1. Total de filas: 62292\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2022-01.parquet\n",
      "Iniciando ingesta de datos de taxis green: 2-2022\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-02.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-02.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2022-02.parquet\n",
      "Ingestando hacia Snowflake green 2022-2. Total de filas: 69207\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2022-02.parquet\n",
      "Iniciando ingesta de datos de taxis green: 3-2022\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-03.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-03.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2022-03.parquet\n",
      "Ingestando hacia Snowflake green 2022-3. Total de filas: 78329\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2022-03.parquet\n",
      "Iniciando ingesta de datos de taxis green: 4-2022\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-04.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-04.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2022-04.parquet\n",
      "Ingestando hacia Snowflake green 2022-4. Total de filas: 75962\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2022-04.parquet\n",
      "Iniciando ingesta de datos de taxis green: 5-2022\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-05.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-05.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2022-05.parquet\n",
      "Ingestando hacia Snowflake green 2022-5. Total de filas: 76725\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2022-05.parquet\n",
      "Iniciando ingesta de datos de taxis green: 6-2022\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-06.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-06.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2022-06.parquet\n",
      "Ingestando hacia Snowflake green 2022-6. Total de filas: 73534\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2022-06.parquet\n",
      "Iniciando ingesta de datos de taxis green: 7-2022\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-07.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-07.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2022-07.parquet\n",
      "Ingestando hacia Snowflake green 2022-7. Total de filas: 64038\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2022-07.parquet\n",
      "Iniciando ingesta de datos de taxis green: 8-2022\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-08.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-08.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2022-08.parquet\n",
      "Ingestando hacia Snowflake green 2022-8. Total de filas: 65776\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2022-08.parquet\n",
      "Iniciando ingesta de datos de taxis green: 9-2022\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-09.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-09.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2022-09.parquet\n",
      "Ingestando hacia Snowflake green 2022-9. Total de filas: 68855\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2022-09.parquet\n",
      "Iniciando ingesta de datos de taxis green: 10-2022\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-10.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-10.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2022-10.parquet\n",
      "Ingestando hacia Snowflake green 2022-10. Total de filas: 69178\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2022-10.parquet\n",
      "Iniciando ingesta de datos de taxis green: 11-2022\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-11.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-11.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2022-11.parquet\n",
      "Ingestando hacia Snowflake green 2022-11. Total de filas: 62152\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2022-11.parquet\n",
      "Iniciando ingesta de datos de taxis green: 12-2022\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-12.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-12.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2022-12.parquet\n",
      "Ingestando hacia Snowflake green 2022-12. Total de filas: 72224\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2022-12.parquet\n",
      "Iniciando ingesta de datos de taxis green: 1-2023\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-01.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-01.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2023-01.parquet\n",
      "Ingestando hacia Snowflake green 2023-1. Total de filas: 68037\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2023-01.parquet\n",
      "Iniciando ingesta de datos de taxis green: 2-2023\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-02.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-02.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2023-02.parquet\n",
      "Ingestando hacia Snowflake green 2023-2. Total de filas: 64622\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2023-02.parquet\n",
      "Iniciando ingesta de datos de taxis green: 3-2023\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-03.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-03.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2023-03.parquet\n",
      "Ingestando hacia Snowflake green 2023-3. Total de filas: 71847\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2023-03.parquet\n",
      "Iniciando ingesta de datos de taxis green: 4-2023\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-04.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-04.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2023-04.parquet\n",
      "Ingestando hacia Snowflake green 2023-4. Total de filas: 65215\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2023-04.parquet\n",
      "Iniciando ingesta de datos de taxis green: 5-2023\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-05.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-05.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2023-05.parquet\n",
      "Ingestando hacia Snowflake green 2023-5. Total de filas: 69001\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2023-05.parquet\n",
      "Iniciando ingesta de datos de taxis green: 6-2023\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-06.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-06.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2023-06.parquet\n",
      "Ingestando hacia Snowflake green 2023-6. Total de filas: 65385\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2023-06.parquet\n",
      "Iniciando ingesta de datos de taxis green: 7-2023\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-07.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-07.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2023-07.parquet\n",
      "Ingestando hacia Snowflake green 2023-7. Total de filas: 61184\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2023-07.parquet\n",
      "Iniciando ingesta de datos de taxis green: 8-2023\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-08.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-08.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2023-08.parquet\n",
      "Ingestando hacia Snowflake green 2023-8. Total de filas: 60459\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2023-08.parquet\n",
      "Iniciando ingesta de datos de taxis green: 9-2023\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-09.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-09.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2023-09.parquet\n",
      "Ingestando hacia Snowflake green 2023-9. Total de filas: 65266\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2023-09.parquet\n",
      "Iniciando ingesta de datos de taxis green: 10-2023\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-10.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-10.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2023-10.parquet\n",
      "Ingestando hacia Snowflake green 2023-10. Total de filas: 65981\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2023-10.parquet\n",
      "Iniciando ingesta de datos de taxis green: 11-2023\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-11.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-11.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2023-11.parquet\n",
      "Ingestando hacia Snowflake green 2023-11. Total de filas: 63838\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2023-11.parquet\n",
      "Iniciando ingesta de datos de taxis green: 12-2023\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-12.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-12.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2023-12.parquet\n",
      "Ingestando hacia Snowflake green 2023-12. Total de filas: 64028\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2023-12.parquet\n",
      "Iniciando ingesta de datos de taxis green: 1-2024\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-01.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-01.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2024-01.parquet\n",
      "Ingestando hacia Snowflake green 2024-1. Total de filas: 56370\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2024-01.parquet\n",
      "Iniciando ingesta de datos de taxis green: 2-2024\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-02.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-02.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2024-02.parquet\n",
      "Ingestando hacia Snowflake green 2024-2. Total de filas: 53405\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2024-02.parquet\n",
      "Iniciando ingesta de datos de taxis green: 3-2024\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-03.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-03.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2024-03.parquet\n",
      "Ingestando hacia Snowflake green 2024-3. Total de filas: 57261\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2024-03.parquet\n",
      "Iniciando ingesta de datos de taxis green: 4-2024\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-04.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-04.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2024-04.parquet\n",
      "Ingestando hacia Snowflake green 2024-4. Total de filas: 56257\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2024-04.parquet\n",
      "Iniciando ingesta de datos de taxis green: 5-2024\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-05.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-05.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2024-05.parquet\n",
      "Ingestando hacia Snowflake green 2024-5. Total de filas: 60778\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2024-05.parquet\n",
      "Iniciando ingesta de datos de taxis green: 6-2024\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-06.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-06.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2024-06.parquet\n",
      "Ingestando hacia Snowflake green 2024-6. Total de filas: 54568\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2024-06.parquet\n",
      "Iniciando ingesta de datos de taxis green: 7-2024\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-07.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-07.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2024-07.parquet\n",
      "Ingestando hacia Snowflake green 2024-7. Total de filas: 51661\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2024-07.parquet\n",
      "Iniciando ingesta de datos de taxis green: 8-2024\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-08.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-08.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2024-08.parquet\n",
      "Ingestando hacia Snowflake green 2024-8. Total de filas: 51589\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2024-08.parquet\n",
      "Iniciando ingesta de datos de taxis green: 9-2024\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-09.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-09.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2024-09.parquet\n",
      "Ingestando hacia Snowflake green 2024-9. Total de filas: 54282\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2024-09.parquet\n",
      "Iniciando ingesta de datos de taxis green: 10-2024\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-10.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-10.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2024-10.parquet\n",
      "Ingestando hacia Snowflake green 2024-10. Total de filas: 55993\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2024-10.parquet\n",
      "Iniciando ingesta de datos de taxis green: 11-2024\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-11.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-11.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2024-11.parquet\n",
      "Ingestando hacia Snowflake green 2024-11. Total de filas: 52079\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2024-11.parquet\n",
      "Iniciando ingesta de datos de taxis green: 12-2024\n",
      "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-12.parquet\n",
      "Archivo obtenido exitosamente de: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-12.parquet\n",
      "Archivo leido exitosamente por Spark: /tmp/green_tripdata_2024-12.parquet\n",
      "Ingestando hacia Snowflake green 2024-12. Total de filas: 53828\n",
      "Archivo parquet temporal removido: /tmp/green_tripdata_2024-12.parquet\n",
      "El proceso de ingesta masiva de taxis NY fue exitoso\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "#Defino funciones tipicas de checkpoint para en caso de fallo no ingestar datos desde cero\n",
    "def save_checkpoint(year, month):\n",
    "    with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "        json.dump({\"year\": year, \"month\": month}, f)\n",
    "\n",
    "def load_checkpoint(CHECKPOINT_FILE):\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {\"year\": 0, \"month\": 0}\n",
    "\n",
    "#Genero arreglo que contendra datos de ingesta para tabla de conteos\n",
    "resultadosGeneralesIngesta=[]\n",
    "\n",
    "try:\n",
    "    #Leo los datos de tipos de taxis, years y months desde variables de entorno\n",
    "    tipos_taxis=os.getenv(\"SERVICES\").split(',')\n",
    "    for tipo_taxi in tipos_taxis:\n",
    "        \n",
    "        lista_years = sorted([int(item) for item in (os.getenv(\"YEARS\").split(','))])\n",
    "        lista_months = sorted([int(item) for item in (os.getenv(\"MONTHS\").split(','))])\n",
    "        #Cargo el checkpoint y en caso de que tenga registros recorto los arreglos de months y years para recorrer desde ultima ingesta exitosa\n",
    "        CHECKPOINT_FILE = f\"checkpointTaxis{tipo_taxi.capitalize()}.json\"\n",
    "        print(CHECKPOINT_FILE)\n",
    "        checkpoint=load_checkpoint(CHECKPOINT_FILE)\n",
    "        print(f\"checkpoint: {checkpoint}\")\n",
    "        \n",
    "        if ( checkpoint != {\"year\": 0, \"month\": 0} and (int(checkpoint[\"month\"]) in lista_months) and (int(checkpoint[\"year\"]) in lista_years)):    \n",
    "            if ( int(checkpoint[\"month\"]) == lista_months[-1] and int(checkpoint[\"year\"]) != lista_years[-1] ):\n",
    "                lista_years= lista_years[lista_years.index(checkpoint[\"year\"])+1:]\n",
    "            elif ( int(checkpoint[\"month\"]) == lista_months[-1] and int(checkpoint[\"year\"]) == lista_years[-1] ): \n",
    "                continue\n",
    "            else:\n",
    "                lista_years= lista_years[lista_years.index(checkpoint[\"year\"]):]\n",
    "                lista_months= lista_months[lista_months.index(checkpoint[\"month\"])+1:]\n",
    "        \n",
    "        for year_taxi in lista_years:\n",
    "            for month_taxi in lista_months:  \n",
    "                #Llamo a la funcion de ingesta de datos iterativamente para cada mes, year y tipo de taxi\n",
    "                print(f\"Iniciando ingesta de datos de taxis {tipo_taxi}: {month_taxi}-{year_taxi}\")\n",
    "                resultadosParciales=ingestar_parquet_a_raw(tipo_taxi, year_taxi, month_taxi)\n",
    "                #Guardo los resultados y genero el checkpint\n",
    "                if (resultadosParciales is not None):\n",
    "                    resultadosGeneralesIngesta.append(resultadosParciales)\n",
    "                    save_checkpoint(year_taxi,month_taxi)\n",
    "            lista_months = sorted([int(item) for item in (os.getenv(\"MONTHS\").split(','))])\n",
    "                \n",
    "except Exception as e5:\n",
    "    #Como en todas las funciones vistas hago manejo de errores\n",
    "    print(f\"Fallo el proceso de ingesta masiva de datos de taxis NY: {e5}\")\n",
    "else:\n",
    "    print(\"El proceso de ingesta masiva de taxis NY fue exitoso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c364cf52-1277-44cd-87a2-82a103a22890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
