{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce09fd9",
   "metadata": {},
   "source": [
    "# Notebook para consumir datos de Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432d999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2160f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "load_dotenv(dotenv_path=os.path.join(os.path.dirname(os.getcwd()), '.env'))\n",
    "\n",
    "def parse_env_list(var_name: str) -> List[str]:\n",
    "    raw = os.getenv(var_name, '')\n",
    "    parts = [p.strip() for p in raw.split(',') if p.strip()]\n",
    "    return parts\n",
    "\n",
    "def parse_date(var_name: str) -> str:\n",
    "    raw = os.getenv(var_name, '')\n",
    "    if not raw:\n",
    "        raise ValueError(f\"Falta variable {var_name} en .env\")\n",
    "    # aceptar formatos dd-mm-yyyy o yyyy-mm-dd\n",
    "    ddmmyyyy = re.fullmatch(r\"(\\d{2})-(\\d{2})-(\\d{4})\", raw)\n",
    "    yyyymmdd = re.fullmatch(r\"(\\d{4})-(\\d{2})-(\\d{2})\", raw)\n",
    "    if ddmmyyyy:\n",
    "        d, m, y = ddmmyyyy.groups()\n",
    "        return f\"{y}-{m}-{d}\"\n",
    "    elif yyyymmdd:\n",
    "        return raw\n",
    "    else:\n",
    "        raise ValueError(f\"Formato de fecha inválido para {var_name}: {raw}\")\n",
    "\n",
    "TICKERS = parse_env_list('TICKERS')\n",
    "START_DATE = parse_date('START_DATE')\n",
    "END_DATE = parse_date('END_DATE')\n",
    "\n",
    "print('Tickers:', TICKERS)\n",
    "print('Fecha inicio:', START_DATE)\n",
    "print('Fecha fin:', END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c7fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import sqlalchemy as sa\n",
    "\n",
    "PG_HOST = os.getenv('POSTGRES_HOST')\n",
    "PG_PORT = os.getenv('POSTGRES_PORT', '5432')\n",
    "PG_DB = os.getenv('POSTGRES_DB')\n",
    "PG_USER = os.getenv('POSTGRES_USER')\n",
    "PG_PASSWORD = os.getenv('POSTGRES_PASSWORD')\n",
    "if not all([PG_HOST, PG_DB, PG_USER, PG_PASSWORD]):\n",
    "    raise RuntimeError('Faltan variables POSTGRES_* en .env')\n",
    "\n",
    "pg_url = f\"postgresql+psycopg2://{PG_USER}:{PG_PASSWORD}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    "engine = sa.create_engine(pg_url)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(sa.text('SELECT 1'))\n",
    "    print('Conexión OK, SELECT 1 ->', list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3a0e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "def download_yf(ticker: str, start: str, end: str) -> pd.DataFrame:\n",
    "    df = yf.download(ticker, start=start, end=end, progress=False, auto_adjust=False)\n",
    "    # Estructura esperada: ['Open','High','Low','Close','Adj Close','Volume'] con index por fecha\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df.rename(columns={\n",
    "        'Open':'open','High':'high','Low':'low','Close':'close','Adj Close':'adj_close','Volume':'volume'\n",
    "    })\n",
    "    df.index = pd.to_datetime(df.index).tz_localize('UTC')\n",
    "    df = df.reset_index().rename(columns={'Date':'date'})\n",
    "    df['ticker'] = ticker\n",
    "    return df[['date','ticker','open','high','low','close','adj_close','volume']]\n",
    "\n",
    "frames = []\n",
    "for t in TICKERS:\n",
    "    dft = download_yf(t, START_DATE, END_DATE)\n",
    "    print(t, 'filas descargadas:', len(dft))\n",
    "    frames.append(dft)\n",
    "\n",
    "raw_df = pd.concat(frames, axis=0, ignore_index=True) if frames else pd.DataFrame(columns=['date','ticker','open','high','low','close','adj_close','volume'])\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab03d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Normalizar y validar datos (fechas, columnas OHLCV)\n",
    "import numpy as np\n",
    "\n",
    "def normalize_validate(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    # Eliminar NaN y duplicados\n",
    "    df = df.dropna(subset=['date','ticker','open','high','low','close','volume'])\n",
    "    df = df.drop_duplicates(subset=['date','ticker'])\n",
    "    # Validaciones básicas\n",
    "    df = df[(df['open'] <= df['high']) & (df['low'] <= df['close'])]\n",
    "    # Asegurar tipos\n",
    "    for col in ['open','high','low','close','adj_close']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df['volume'] = pd.to_numeric(df['volume'], errors='coerce').astype('Int64')\n",
    "    df['date'] = pd.to_datetime(df['date'], utc=True)\n",
    "    return df\n",
    "\n",
    "clean_df = normalize_validate(raw_df)\n",
    "print('Total filas tras limpieza:', len(clean_df))\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3d8d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Insertar/Upsert en tabla 01-init-schemas\n",
    "from sqlalchemy import text\n",
    "\n",
    "# Configuración de esquema/tabla destino\n",
    "DEST_SCHEMA = os.getenv('RAW_SCHEMA', 'raw')\n",
    "DEST_TABLE = os.getenv('RAW_TABLE', 'yf_prices')\n",
    "FULL_TABLE = f\"{DEST_SCHEMA}.{DEST_TABLE}\"\n",
    "\n",
    "create_sql = f'''\n",
    "CREATE TABLE IF NOT EXISTS {FULL_TABLE} (\n",
    "    date TIMESTAMPTZ NOT NULL,\n",
    "    ticker TEXT NOT NULL,\n",
    "    open DOUBLE PRECISION,\n",
    "    high DOUBLE PRECISION,\n",
    "    low DOUBLE PRECISION,\n",
    "    close DOUBLE PRECISION,\n",
    "    adj_close DOUBLE PRECISION,\n",
    "    volume BIGINT,\n",
    "    PRIMARY KEY(date, ticker)\n",
    ");\n",
    "'''\n",
    "\n",
    "upsert_sql = f'''\n",
    "INSERT INTO {FULL_TABLE} (date, ticker, open, high, low, close, adj_close, volume)\n",
    "VALUES (:date, :ticker, :open, :high, :low, :close, :adj_close, :volume)\n",
    "ON CONFLICT (date, ticker) DO UPDATE SET\n",
    "    open = EXCLUDED.open,\n",
    "    high = EXCLUDED.high,\n",
    "    low = EXCLUDED.low,\n",
    "    close = EXCLUDED.close,\n",
    "    adj_close = EXCLUDED.adj_close,\n",
    "    volume = EXCLUDED.volume;\n",
    "'''\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(create_sql))\n",
    "    # Inserción por lotes\n",
    "    batch = clean_df.to_dict('records')\n",
    "    if batch:\n",
    "        conn.execute(text(upsert_sql), batch)\n",
    "        print('Upsert realizado:', len(batch), 'filas')\n",
    "    else:\n",
    "        print('No hay filas para insertar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862e3d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Ejecutar en VS Code: salida en Output y verificación con consultas\n",
    "# Métricas por ticker\n",
    "summary = clean_df.groupby('ticker')['date'].count().rename('rows').reset_index()\n",
    "print(summary)\n",
    "\n",
    "# Consulta de verificación: COUNT(*) por año y ticker\n",
    "with engine.connect() as conn:\n",
    "    check = conn.execute(text(f\"\"\"\n",
    "        SELECT ticker, EXTRACT(YEAR FROM date) AS year, COUNT(*) AS rows\n",
    "        FROM {FULL_TABLE}\n",
    "        GROUP BY ticker, EXTRACT(YEAR FROM date)\n",
    "        ORDER BY ticker, year\n",
    "    \"\"\"))\n",
    "    print('Verificación en Postgres:')\n",
    "    for row in check:\n",
    "        print(dict(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6edc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Pruebas rápidas de conectividad y esquema\n",
    "import pytest\n",
    "from sqlalchemy import text\n",
    "\n",
    "def test_pg_connection():\n",
    "    with engine.connect() as conn:\n",
    "        res = conn.execute(text('SELECT 1')).scalar()\n",
    "        assert res == 1\n",
    "\n",
    "def test_table_columns():\n",
    "    expected = {'date','ticker','open','high','low','close','adj_close','volume'}\n",
    "    with engine.connect() as conn:\n",
    "        cols = conn.execute(text(\"SELECT column_name FROM information_schema.columns WHERE table_schema=:s AND table_name=:t\"),\n",
    "                            {'s': DEST_SCHEMA, 't': DEST_TABLE}).scalars().all()\n",
    "        assert expected.issubset(set(cols)), f\"Faltan columnas en {FULL_TABLE}: {expected - set(cols)}\"\n",
    "\n",
    "print('Pruebas simples OK si no hubo assert.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1790e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función estilo ingestar_zones_a_raw para Yahoo Finance -> Postgres\n",
    "import os\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def ingestar_yf_a_raw():\n",
    "    DEST_SCHEMA = os.getenv('RAW_SCHEMA', 'raw')\n",
    "    DEST_TABLE = os.getenv('RAW_TABLE', 'yf_prices')\n",
    "    FULL_TABLE = f\"{DEST_SCHEMA}.{DEST_TABLE}\"\n",
    "    POSTGRES_DB = os.getenv('POSTGRES_DB')\n",
    "    POSTGRES_USER = os.getenv('POSTGRES_USER')\n",
    "    POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD')\n",
    "    POSTGRES_HOST = os.getenv('POSTGRES_HOST', 'warehouses')\n",
    "    POSTGRES_PORT = os.getenv('POSTGRES_PORT', '5432')\n",
    "    \n",
    "    # Asegurar Spark activo\n",
    "    try:\n",
    "        spark.version\n",
    "    except Exception:\n",
    "        jar_path = os.getenv('POSTGRES_JAR_PATH', '/home/jovyan/work/postgresql-42.2.5.jar')\n",
    "        spark = SparkSession.builder.config('spark.jars', jar_path).master('local').appName('YF_Postgres_Ingest').getOrCreate()\n",
    "    \n",
    "    tickers = [t.strip() for t in os.getenv('TICKERS','').split(',') if t.strip()]\n",
    "    start = os.getenv('START_DATE')\n",
    "    end = os.getenv('END_DATE')\n",
    "    if not tickers or not start or not end:\n",
    "        print('Faltan TICKERS/START_DATE/END_DATE en .env')\n",
    "        return None\n",
    "    \n",
    "    total_rows = 0\n",
    "    for ticker in tickers:\n",
    "        local_path = f\"/tmp/yf_{ticker}.csv\"\n",
    "        try:\n",
    "            df = yf.download(ticker, start=start, end=end, progress=False, auto_adjust=False)\n",
    "            if df.empty:\n",
    "                print(f'Sin datos para {ticker}')\n",
    "                continue\n",
    "            df = df.rename(columns={'Open':'open','High':'high','Low':'low','Close':'close','Adj Close':'adj_close','Volume':'volume'})\n",
    "            df.index = pd.to_datetime(df.index).tz_localize('UTC')\n",
    "            df.reset_index().rename(columns={'Date':'date'})[['Date','open','high','low','close','adj_close','volume']].rename(columns={'Date':'date'}).assign(ticker=ticker).to_csv(local_path, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error obteniendo datos de YF para {ticker}: {e}\")\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"Archivo obtenido exitosamente para {ticker} -> {local_path}\")\n",
    "        \n",
    "        # Leo el archivo en un df de Spark\n",
    "        try:\n",
    "            sdf = spark.read.csv(local_path, header='true', inferSchema='true')\n",
    "        except Exception as e:\n",
    "            print(f\"No se pudo leer {local_path}: {e}\")\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"Archivo leído exitosamente por Spark: {local_path}\")\n",
    "        \n",
    "        conteoFilas = sdf.count()\n",
    "        print(f\"Ingestando hacia Postgres datos de {ticker}. Total de filas: {conteoFilas}\")\n",
    "        \n",
    "        try:\n",
    "            sdf.write.format('jdbc') \\\n",
    "                .option('url', f\"jdbc:postgresql://{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}\") \\\n",
    "                .option('driver', 'org.postgresql.Driver') \\\n",
    "                .option('dbtable', FULL_TABLE) \\\n",
    "                .option('user', POSTGRES_USER) \\\n",
    "                .option('password', POSTGRES_PASSWORD) \\\n",
    "                .mode('append') \\\n",
    "                .save()\n",
    "        except Exception as e2:\n",
    "            print(f\"Error con ingreso de datos: {e2}\")\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"Datos de {ticker} exportados correctamente a {FULL_TABLE}\")\n",
    "            total_rows += conteoFilas\n",
    "        \n",
    "        # Eliminar archivo temporal\n",
    "        try:\n",
    "            os.remove(local_path)\n",
    "            print(f\"Archivo temporal removido: {local_path}\")\n",
    "        except OSError as e:\n",
    "            print(f\"No se pudo remover el archivo temporal {local_path}: {e}\")\n",
    "    \n",
    "    return {\n",
    "        'count': total_rows,\n",
    "        'tickers': tickers,\n",
    "        'table': FULL_TABLE,\n",
    "    }\n",
    "\n",
    "resultado = ingestar_yf_a_raw()\n",
    "print('Resultado:', resultado)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
